@Article{ROCR,
    entry = {article},
    title = {ROCR: visualizing classifier performance in R},
    author = {T. Sing and O. Sander and N. Beerenwinkel and T. Lengauer},
    year = {2005},
    journal = {Bioinformatics},
    volume = {21},
    number = {20},
    pages = {7881},
    doi = {10.1093/bioinformatics/bti623},
    url = {http://rocr.bioinf.mpi-sb.mpg.de},
  }

@article{seewave2008,
    title = {Seewave: a free modular tool for sound analysis and synthesis},
    author = {{J. Sueur} and {T. Aubin} and {C. Simonis}},
    year = {2008},
    journal = {Bioacoustics},
    volume = {18},
    pages = {213-226},
    doi = {10.1080/09524622.2008.9753600}
  }

@article{clink2019gibbonfindr,
  title={gibbonR: An R package for the detection and classification of acoustic signals},
  author={Clink, D. J. and Klinck, Holger},
  journal={arXiv preprint arXiv:1906.02572},
  year={2019},
  doi={https://doi.org/10.48550/arXiv.1906.02572}
}

@article{clinkzenodo2024,
  author       = {Clink, D. J. and
                  Hamid Ahmad, Abdul},
  title        = {A labelled dataset of the loud calls of four
                   vertebrates collected using passive acoustic
                   monitoring in Malaysian Borneo
                  },
  month        = nov,
  year         = 2024,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.14213067},
  url          = {https://doi.org/10.5281/zenodo.14213067},
}

@article{araya2017warbler,
  title={warbleR: an R package to streamline analysis of animal acoustic signals},
  author={Araya-Salas, Marcelo and Smith-Vidaurre, Grace},
  journal={Methods in Ecology and Evolution},
  volume={8},
  number={2},
  pages={184--191},
  year={2017},
  publisher={Wiley Online Library},
  doi={https://dx.doi.org/10.1111/2041-210X.12624}
}

@article{clink2023,
	title = {A workflow for the automated detection and classification of female gibbon calls from long-term acoustic recordings},
	author = {Clink, D. J. and Kier, Isabel and Ahmad, Abdul Hamid and Klinck, Holger},
	year = {2023},
	date = {2023},
	journal = {Frontiers in Ecology and Evolution},
	volume = {11},
	doi = {10.3389/fevo.2023.1071640}
}

@article{sugai2019,
	title = {Terrestrial Passive Acoustic Monitoring: Review and Perspectives},
	author = {Sugai, Larissa Sayuri Moreira and Silva, Thiago Sanna Freire and Ribeiro, {José Wagner} and Llusia, Diego},
	year = {2019},
	date = {2019},
	journal = {BioScience},
	pages = {15{\textendash}25},
	volume = {69},
	number = {1},
	doi = {10.1093/biosci/biy147},
	url = {https://academic.oup.com/bioscience/article/69/1/15/5193506},
	note = {Publisher: Narnia}
}

@article{gibb2018,
	title = {Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring},
	author = {Gibb, Rory and Browning, Ella and Glover-Kapfer, Paul and Jones, Kate E.},
	year = {2018},
	month = {10},
	date = {2018-10},
	journal = {Methods in Ecology and Evolution},
	doi = {10.1111/2041-210X.13101},
	url = {http://doi.wiley.com/10.1111/2041-210X.13101},
	note = {Publisher: Wiley/Blackwell (10.1111)}
}

@article{katz2016,
	title = {Assessment of error rates in acoustic monitoring with the R package monitoR},
	author = {Katz, Jonathan and Hafner, Sasha D and Donovan, Therese},
	year = {2016},
	date = {2016},
	journal = {Bioacoustics},
	pages = {177{\textendash}196},
	volume = {25},
	number = {2},
	note = {Publisher: Taylor & Francis},
	doi= {10.1080/09524622.2015.1133320}
}

@article{balantic2020,
	title = {AMMonitor: Remote monitoring of biodiversity in an adaptive framework with r},
	author = {Balantic, Cathleen and Donovan, Therese},
	year = {2020},
	date = {2020},
	journal = {Methods in Ecology and Evolution},
	pages = {869{\textendash}877},
	volume = {11},
	number = {7},
	note = {Publisher: Wiley Online Library},
	doi={https://doi.org/10.1111/2041-210X.13397}
}

@article{kalan2015,
	title = {Towards the automated detection and occupancy estimation of primates using passive acoustic monitoring},
	author = {Kalan, Ammie K. and Mundry, Roger and Wagner, Oliver J J and Heinicke, Stefanie and Boesch, Christophe and {Kühl}, Hjalmar S.},
	year = {2015},
	date = {2015},
	journal = {Ecological Indicators},
	pages = {217{\textendash}226},
	volume = {54},
	number = {July 2015},
	doi = {10.1016/j.ecolind.2015.02.023}
}

@article{lecun2015,
	title = {Deep learning},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	month = {05},
	date = {2015-05},
	journal = {Nature},
	pages = {436--444},
	volume = {521},
	number = {7553},
	doi = {10.1038/nature14539},
	url = {https://www.nature.com/articles/nature14539},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	langid = {en}
}

@book{scavetta2021,
	title = {Python and R for the Modern Data Scientist},
	author = {Scavetta, Rick J and Angelov, Boyan},
	year = {2021},
	date = {2021},
	publisher = {O'Reilly Media, Inc.},
	doi={10.18637/jss.v103.b02}
}

@article{lawlor2022,
	title = {Ten simple rules for teaching yourself R},
	author = {Lawlor, Jake and Banville, Francis and {Forero-Muñoz}, Norma-Rocio and {Hébert}, Katherine and {Martínez-Lanfranco}, {Juan Andrés} and Rogy, Pierre and MacDonald, A. Andrew M.},
	year = {2022},
	month = {09},
	date = {2022-09-01},
	journal = {PLOS Computational Biology},
	pages = {e1010372},
	volume = {18},
	number = {9},
	doi = {10.1371/journal.pcbi.1010372},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010372},
	note = {Publisher: Public Library of Science},
	langid = {en}
}

@article{chollet2015,
	title = {Keras},
	author = {Chollet, {François} and others, },
	year = {2015},
	date = {2015},
	url = {https://github.com/fchollet/keras},
	doi = {10.1163/1574-9347_bnp_e612900}
}

@inbook{paszke2019,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	date = {2019},
	publisher = {Curran Associates, Inc.},
	pages = {8024{\textendash}8035},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	doi={https://doi.org/10.48550/arXiv.1912.01703}
}

@article{martínabadi2015,
	title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
	author = {{Martín Abadi},  and Ashish Agarwal,  and Paul Barham,  and Eugene Brevdo,  and Zhifeng Chen,  and Craig Citro,  and Greg S. Corrado,  and Andy Davis,  and Jeffrey Dean,  and Matthieu Devin,  and Sanjay Ghemawat,  and Ian Goodfellow,  and Andrew Harp,  and Geoffrey Irving,  and Michael Isard,  and Jia, Yangqing and Rafal Jozefowicz,  and Lukasz Kaiser,  and Manjunath Kudlur,  and Josh Levenberg,  and {Dandelion Mané},  and Rajat Monga,  and Sherry Moore,  and Derek Murray,  and Chris Olah,  and Mike Schuster,  and Jonathon Shlens,  and Benoit Steiner,  and Ilya Sutskever,  and Kunal Talwar,  and Paul Tucker,  and Vincent Vanhoucke,  and Vijay Vasudevan,  and {Fernanda Viégas},  and Oriol Vinyals,  and Pete Warden,  and Martin Wattenberg,  and Martin Wicke,  and Yuan Yu,  and Xiaoqiang Zheng, },
	year = {2015},
	date = {2015},
	url = {https://www.tensorflow.org/},
	doi ={https://doi.org/10.48550/arXiv.1605.08695}
}

@book{ushey2022,
	title = {reticulate: Interface to 'Python'},
	author = {Ushey, Kevin and Allaire, J. J. and Tang, Yuan},
	year = {2022},
	date = {2022},
	doi={10.32614/CRAN.package.reticulat}
}

@book{falbel2023,
	title = {luz: Higher Level 'API' for 'torch'},
	author = {Falbel, Daniel},
	year = {2023},
	date = {2023},
	url = {https://CRAN.R-project.org/package=luz}
}

@book{stevens2020,
	title = {Deep Learning with PyTorch},
	author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
	year = {2020},
	month = {08},
	date = {2020-08-04},
	publisher = {Simon and Schuster},
	note = {Google-Books-ID: fff1DwAAQBAJ},
	langid = {en}
}

@book{goodfellow2016,
	title = {Deep Learning},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	date = {2016},
	publisher = {MIT Press}
}

@inproceedings{deng2009,
	title = {Imagenet: A large-scale hierarchical image database},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	date = {2009},
	publisher = {Ieee},
	pages = {248{\textendash}255},
	doi = {10.1109/cvpr.2009.5206848}
}

@article{dufourq2022,
	title = {Passive acoustic monitoring of animal populations with transfer learning},
	author = {Dufourq, Emmanuel and Batist, Carly and Foquet, Ruben and Durbach, Ian},
	year = {2022},
	date = {2022},
	journal = {Ecological Informatics},
	pages = {101688},
	volume = {70},
	doi = {10.1016/j.ecoinf.2022.101688},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954122001388}
}

@article{ruan2022,
	title = {ResNet-based bio-acoustics presence detection technology of Hainan gibbon calls},
	author = {Ruan, Wenda and Wu, Keyi and Chen, Qingchun and Zhang, Chengyun},
	year = {2022},
	date = {2022},
	journal = {Applied Acoustics},
	pages = {108939},
	volume = {198},
	doi = {10.1016/j.apacoust.2022.108939},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X22003139}
}

@inproceedings{tan2018,
	title = {A survey on deep transfer learning},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year = {2018},
	date = {2018},
	publisher = {Springer},
	pages = {270{\textendash}279}
}

@article{weiss2016,
	title = {A survey of transfer learning},
	author = {Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
	year = {2016},
	date = {2016},
	journal = {Journal of Big data},
	pages = {1{\textendash}40},
	volume = {3},
	number = {1},
	note = {Publisher: SpringerOpen}
}

@book{keydana2023,
	title = {Deep Learning and Scientific Computing with R torch},
	author = {Keydana, Sigrid},
	year = {2023},
	date = {2023},
	publisher = {CRC Press},
	doi = {10.1201/9781003275923}
}

@article{lecun1995,
	title = {Convolutional networks for images, speech, and time series},
	author = {LeCun, Yann and Bengio, Yoshua and others, },
	year = {1995},
	date = {1995},
	journal = {The handbook of brain theory and neural networks},
	pages = {1995},
	volume = {3361},
	number = {10},
	note = {Publisher: Citeseer}
}

@article{gu2018,
	title = {Recent advances in convolutional neural networks},
	author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and others, },
	year = {2018},
	date = {2018},
	journal = {Pattern recognition},
	pages = {354{\textendash}377},
	volume = {77},
	note = {Publisher: Elsevier}
}

@article{krizhevsky2017,
	title = {Imagenet classification with deep convolutional neural networks},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2017},
	date = {2017},
	journal = {Communications of the ACM},
	pages = {84{\textendash}90},
	volume = {60},
	number = {6},
	note = {Publisher: AcM New York, NY, USA},
	doi ={10.1145/3065386}
}

@article{simonyan2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	date = {2014},
	journal = {arXiv preprint arXiv:1409.1556},
	doi={10.48550/arXiv.1409.1556}
}

@inproceedings{he2016,
	title = {Deep residual learning for image recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	date = {2016},
	pages = {770{\textendash}778},
	doi ={10.1109/cvpr.2016.90}
}

@article{lakdari2024mel,
  title={Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons},
  author={Lakdari, Mohamed Walid and Ahmad, Abdul Hamid and Sethi, Sarab and Bohn, Gabriel A and Clink, D. J.},
  journal={Ecological Informatics},
  volume={80},
  pages={102457},
  year={2024},
  publisher={Elsevier},
  doi ={10.1016/j.ecoinf.2023.102457}
}

@article{sethi2020characterizing,
  title={Characterizing soundscapes across diverse ecosystems using a universal acoustic feature set},
  author={Sethi, Sarab S and Jones, Nick S and Fulcher, Ben D and Picinali, Lorenzo and Clink, D. J. and Klinck, Holger and Orme, C David L and Wrege, Peter H and Ewers, Robert M},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={29},
  pages={17049--17055},
  year={2020},
  publisher={National Acad Sciences},
  doi={10.1073/pnas.2004702117}
}

@article{ghani2023,
	title = {Global birdsong embeddings enable superior transfer learning for bioacoustic classification},
	author = {Ghani, Burooj and Denton, Tom and Kahl, Stefan and Klinck, Holger},
	year = {2023},
	date = {2023},
	journal = {Scientific Reports},
	pages = {22876},
	volume = {13},
	number = {1},
	note = {Publisher: Nature Publishing Group UK London},
	doi={10.1038/s41598-023-49989-z}
}

@article{clink2021not,
  title={Not by the light of the moon: investigating circadian rhythms and environmental predictors of calling in Bornean great argus},
  author={Clink, D. J. and Groves, Tom and Ahmad, Abdul Hamid and Klinck, Holger},
  journal={Plos one},
  volume={16},
  number={2},
  pages={e0246564},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA},
  doi = {10.1371/journal.pone.0246564}
}

@article{kennedy2023evidence,
  title={Evidence for acoustic niche partitioning depends on the temporal scale in two sympatric Bornean hornbill species},
  author={Kennedy, Amy G and Ahmad, Abdul Hamid and Klinck, Holger and Johnson, Lynn M and Clink, D. J.},
  journal={Biotropica},
  volume={55},
  number={2},
  pages={517--528},
  year={2023},
  publisher={Wiley Online Library},
  doi={10.1111/btp.13205}
}

@article{stowell2022,
	title = {Computational bioacoustics with deep learning: a review and roadmap},
	author = {Stowell, Dan},
	year = {2022},
	month = {03},
	date = {2022-03-21},
	journal = {PeerJ},
	pages = {e13152},
	volume = {10},
	doi = {10.7717/peerj.13152},
	url = {https://peerj.com/articles/13152},
	note = {Publisher: PeerJ Inc.},
	langid = {en}
}
@article{10.1371/journal.pone.0283396,
    doi = {10.1371/journal.pone.0283396},
    author = {Best, Paul and Paris, Sébastien and Glotin, Hervé and Marxer, Ricard},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Deep audio embeddings for vocalisation clustering},
    year = {2023},
    month = {07},
    volume = {18},
    url = {10.1371/journal.pone.0283396},
    pages = {1-18},
    abstract = {The study of non-human animals’ communication systems generally relies on the transcription of vocal sequences using a finite set of discrete units. This set is referred to as a vocal repertoire, which is specific to a species or a sub-group of a species. When conducted by human experts, the formal description of vocal repertoires can be laborious and/or biased. This motivates computerised assistance for this procedure, for which machine learning algorithms represent a good opportunity. Unsupervised clustering algorithms are suited for grouping close points together, provided a relevant representation. This paper therefore studies a new method for encoding vocalisations, allowing for automatic clustering to alleviate vocal repertoire characterisation. Borrowing from deep representation learning, we use a convolutional auto-encoder network to learn an abstract representation of vocalisations. We report on the quality of the learnt representation, as well as of state of the art methods, by quantifying their agreement with expert labelled vocalisation types from 8 datasets of other studies across 6 species (birds and marine mammals). With this benchmark, we demonstrate that using auto-encoders improves the relevance of vocalisation representation which serves repertoire characterisation using a very limited number of settings. We also publish a Python package for the bioacoustic community to train their own vocalisation auto-encoders or use a pretrained encoder to browse vocal repertoires and ease unit wise annotation.},
    number = {7},

}
@article{LAKDARI2024102457,
title = {Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons},
journal = {Ecological Informatics},
volume = {80},
pages = {102457},
year = {2024},
issn = {1574-9541},
doi = {10.1016/j.ecoinf.2023.102457},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004867},
author = {Lakdari,Mohamed Walid  and Ahmad,Abdul Hamid and Sethi, Sarab and Bohn, Gabriel A.  and Clink, D. J.},
keywords = {Vocal individuality, Sound feature extraction, Mel-frequency cepstral coefficients, Convolutional neural networks, Acoustic indices, },
abstract = {Passive acoustic monitoring – an approach that utilizes autonomous acoustic recording units – allows for non-invasive monitoring of individuals, assuming that it is possible to acoustically distinguish individuals. However, identifying effective analytical approaches for individual identification remains a challenge. Our study investigates how the use of different feature representations impacts our ability to distinguish between individual female Northern grey gibbons (Hylobates funereus). We broadcast pre-recorded calls from twelve gibbon females and re-recorded the calls at varying distances (directly under the tree to ∼400 m away) using autonomous recording units. We evaluated the effectiveness of using different automated feature extraction approaches to classify gibbon calls: Mel-frequency cepstral coefficients (MFCCs), embeddings from three pre-trained neural networks (BirdNET, VGGish, and Wav2Vec2), and four commonly used acoustic indices. We used a supervised classification approach (random forest) to classify calls to the respective female and compared two unsupervised clustering approaches (affinity propagation clustering and hierarchical density-based spatial clustering) to evaluate which features were most effective for distinguishing female calls without using class labels. We used MFCCs as a baseline as previous work has shown they can be used to distinguish high-quality calls of individual gibbon females. Human annotators could only identify calls in spectrograms from recordings <350 m from the playback speaker with signal-to-noise ratio ∼ 0 dB, so our results focus on these recordings. Using supervised classification, our results confirmed the efficiency of MFCCs and the use of embeddings from one neural network (BirdNET) for effective acoustic classification of gibbon individuals at closer recording distances (signal-to-noise ratio > 10 dB), while the remaining features did not perform well. Contrary to our expectations, we found that MFCCs outperformed all other features for the unsupervised clustering tasks at closer distances and none of the features performed well at farther distances. The ability to acoustically discriminate animals under noisy conditions and from low signal-to-noise ratio calls has important implications for monitoring populations of endangered animals, such as gibbons. Focusing only on high signal-to-noise ratio calls for individual discrimination may not be possible for rare sounds, and future work should focus on developing effective approaches of feature extraction that can perform well across noisy, real-world conditions with a limited number of training samples.}
}

@Article{dbscan,
  title = {{dbscan}: Fast Density-Based Clustering with {R}},
  author = {Michael Hahsler and Matthew Piekenbrock and Derek Doran},
  journal = {Journal of Statistical Software},
  year = {2019},
  volume = {91},
  number = {1},
  pages = {1--30},
  doi = {10.18637/jss.v091.i01},
}

@Article{kuhncaret,
  title = {Building Predictive Models in R Using the caret Package},
  volume = {28},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v028i05},
  doi = {10.18637/jss.v028.i05},
  number = {5},
  journal = {Journal of Statistical Software},
  author = {{Kuhn} and {Max}},
  year = {2008},
  pages = {1–26},
}

@article{kuhn2008,
	title = {Caret package},
	author = {Kuhn, Max},
	year = {2008},
	date = {2008},
	journal = {Journal of Statistical Software},
	pages = {1{\textendash}26},
	volume = {28},
	number = {5},
	doi = {10.18637/jss.v028.i05}
}
@Manual{umap,
  title = {umap: Uniform Manifold Approximation and Projection},
  author = {Tomasz Konopka},
  year = {2023},
  note = {R package version 0.2.10.0},
  url = {https://CRAN.R-project.org/package=umap},
  doi={	10.32614/CRAN.package.umap}
}

@article{ruff2021,
	title = {Workflow and convolutional neural network for automated identification of animal sounds},
	author = {Ruff, Zachary J. and Lesmeister, Damon B. and Appel, Cara L. and Sullivan, Christopher M.},
	year = {2021},
	month = {05},
	date = {2021-05-01},
	journal = {Ecological Indicators},
	pages = {107419},
	volume = {124},
	doi = {10.1016/j.ecolind.2021.107419},
	url = {https://www.sciencedirect.com/science/article/pii/S1470160X21000844},
	langid = {en}
}

@book{ruff2020,
	title = {Convolutional neural network and R-Shiny app for identifying vocalizations in Pacific Northwest forests},
	author = {Ruff, Zachary J. and Lesmeister, Damon B. and Appel, Cara L. and Sullivan, Christopher M.},
	year = {2020},
	month = {10},
	date = {2020-10},
	publisher = {Zenodo},
	doi = {10.5281/zenodo.4092393},
	url = {10.5281/zenodo.4092393},
	note = {DOI: 10.5281/zenodo.4092393}
}

@article{silva2022soundclass,
  title={soundClass: An automatic sound classification tool for biodiversity monitoring using machine learning},
  author={Silva, Bruno and Mestre, Frederico and Barreiro, Sílvia and Alves, Pedro J and Herrera, José M},
  journal={Methods in Ecology and Evolution},
  year={2022},
  doi={10.1111/2041-210X.13964}
}

@article {Clink2024benchmark,
	author = {Clink, D. J. and Cross-Jaya, Hope and Kim, Jinsung and Ahmad, Abdul Hamid and Hong, Moeurk and Sala, Roeun and Birot, H{\'e}l{\`e}ne and Agger, Cain and Vu, Thinh Tien and Thi, Hoa Nguyen and Chi, Thanh Nguyen and Klinck, Holger},
	title = {Benchmarking for the automated detection and classification of southern yellow-cheeked crested gibbon calls from passive acoustic monitoring data},
	elocation-id = {2024.08.17.608420},
	year = {2024},
	doi = {10.1101/2024.08.17.608420},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Recent advances in deep and transfer learning have revolutionized our ability for the automated detection and classification of acoustic signals from long-term recordings. Here, we provide a benchmark for the automated detection of southern yellow-cheeked crested gibbon (Nomascus gabriellae) calls collected using autonomous recording units (ARUs) in Andoung Kraleung Village, Cambodia. We compared the performance of support vector machines (SVMs), a quasi-DenseNet architecture (Koogu), transfer learning with pretrained convolutional neural network (ResNet50) models trained on the ImageNet dataset, and transfer learning with embeddings from a global birdsong model (BirdNET) based on an EfficientNet architecture. We also investigated the impact of varying the number of training samples on the performance of these models. We found that BirdNET had superior performance with a smaller number of training samples, whereas Koogu and ResNet50 models only had acceptable performance with a larger number of training samples (\&gt;200 gibbon samples). Effective automated detection approaches are critical for monitoring endangered species, like gibbons. Future work on other gibbon species will be informative. Code and data are publicly available for future benchmarking.},
	URL = {https://www.biorxiv.org/content/early/2024/08/19/2024.08.17.608420},
	eprint = {https://www.biorxiv.org/content/early/2024/08/19/2024.08.17.608420.full.pdf},
	journal = {bioRxiv}
}

@article{clink2024automated,
  title={Automated detection of gibbon calls from passive acoustic monitoring data using convolutional neural networks in the" torch for R" ecosystem},
  author={Clink, D. J. and Kim, Jinsung and Cross-Jaya, Hope and Ahmad, Abdul Hamid and Hong, Moeurk and Sala, Roeun and Birot, H{\'e}l{\`e}ne and Agger, Cain and Vu, Thinh Tien and Thi, Hoa Nguyen and others},
  journal={arXiv preprint arXiv:2407.09976},
  year={2024}
}

@article{Vu2024,
  title={Investigating hunting in a protected area in Southeast Asia using passive acoustic monitoring with mobile smartphones and transfer learning},
  author={Vu, Thinh Tien and Phan, Dai Viet and Le, Thai Son and Clink, D. J.},
  journal={Ecological Indicators},
  year={2024},
  note={Forthcoming},
  url={10.1016/j.ecolind.2024.112501},
}

@misc{pytorch_quantized_transfer_learning,
  author       = {Takhirov, Zafar},
  title        = {Quantized Transfer Learning Tutorial},
  year         = 2021,
  url          = {https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html},
  note         = {Accessed: 2025-02-15}
}



