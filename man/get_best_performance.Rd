% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_best_performance.R
\name{get_best_performance}
\alias{get_best_performance}
\title{Extract Best Performance Results from Performance Tables}
\usage{
get_best_performance(
  performancetables.dir,
  model.type = "multi",
  class = "hornbill.helmeted",
  Thresh.val = 0.5
)
}
\arguments{
\item{performancetables.dir}{Path to the directory containing the performance tables.}

\item{model.type}{Type of model architecture. If 'multi' then will treat as multiclass, otherwise will treat as binary.}

\item{class}{Specific class for evaluation.}

\item{Thresh.val}{Threshold value for evaluation.}
}
\value{
A list containing best F1 scores, best precision results, best recall results,
and plots visualizing these metrics.
}
\description{
Given the path to a directory of performance tables, this function reads in the tables,
combines them, and extracts the best performance results based on various criteria.
}
\examples{
{
# Simulate data for performance tables
set.seed(123)

#' Set directory
performance_tables_dir <- paste(tempdir(),"/example_performance_tables/", sep='')

#' Create directory for performance tables (NOTE THIS IS FOR TESTING ONLY)
dir.create(performance_tables_dir, showWarnings = FALSE, recursive = TRUE)

#' Define list of model architectures
architectures <- c("alexnet", "vgg16", "vgg19")

#' Define list of training datasets
training_datasets <- c("Dataset1", "Dataset2", "Dataset3")

#' Create performance tables
for (arch in architectures) {
  for (td in training_datasets) {
    #' Generate random performance metrics
    metrics <- data.frame(
      Class = rep(c("hornbill.helmeted", "other.class"), each = 5),
      "Training Data" = rep(td, 10),
      "CNN Architecture" = rep(arch, 10),
      Threshold = runif(10, 0, 1),
      F1 = runif(10, 0, 1),
      Precision = runif(10, 0, 1),
      Recall = runif(10, 0, 1),
      AUC = runif(10, 0, 1),
      `N epochs` = rep(c(10, 20, 30), each = 10)
    )

   # Reassign column names
    colnames(metrics) <- c("Class",
    "Training Data", "CNN Architecture",
    "Threshold", "F1", "Precision",
    "Recall", "AUC", "N epochs")
    #' Write data to CSV file
    filename <- paste0(performance_tables_dir, arch, "_", td, ".csv")
    write.csv(metrics, filename, row.names = FALSE)
  }
}


#' Call the function with default parameters
results <- get_best_performance(performancetables.dir = performance_tables_dir, )

 # NOTE: Results will not make sense as it is random
#' Print the best F1 scores
print("Best F1 scores:")
print(results$best_f1)

#' Print the best precision results
print("Best precision results:")
print(results$best_precision)

#' Print the best recall results
print("Best recall results:")
print(results$best_recall)

#' Print the best AUC results
print("Best AUC results:")
print(results$best_auc)

#' Plot F1 scores
print(results$f1_plot)

#' Plot precision-recall curve
print(results$pr_plot)
}
}
