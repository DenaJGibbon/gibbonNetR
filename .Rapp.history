# Prepare data ------------------------------------------------------------#
  # Load required libraries  library(luz)                # Luz library for deep learning with PyTorch  library(torch)              # PyTorch library  library(torchvision)        # PyTorch library for computer vision tasks  library(torchdatasets)      # PyTorch library for handling datasets#
  library(stringr)            # String manipulation library  library(tuneR)              # Library for audio analysis  library(seewave)            # Library for sound analysis  #library(gibbonR)            # Library for bioacoustic analysis#
  # NEED TO ADD:  metadata output#
  # Set the output folder paths  OutputFolder <- '/Users/denaclink/Desktop/JahooArray/ResNet50Unfrozen/FemaleDetections/'  OutputFolderSelections <- '/Users/denaclink/Desktop/JahooArray/ResNet50Unfrozen/FemaleSelectionTables/'  OutputFolderWav <- '/Users/denaclink/Desktop/JahooArray/ResNet50Unfrozen/FemaleWavs/'#
  # Import trained model  # Chose because had highest precision while maintaining recall   modelAlexNetGibbon <- luz_load("/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin/_imagescambodiaFP_4_modelResNet50.pt")#
  # path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustic Gibbon PAM 27_09_22' # Do not run this again because T/F positives split#
  path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM 15_03_22'  # Already run above this line ##
  # path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM 15_12_22'#
  # Get a list of full file paths of sound files in a directory  SoundFilePathFull <- list.files(path.to.files,                                  recursive = T, pattern = '.wav', full.names = T)#
  # Get a list of file names of sound files in a directory  SoundFilePathShort <- list.files(path.to.files,                                   recursive = T, pattern = '.wav')#
  # Extract only the file names without the extension  SoundFilePathShort <- str_split_fixed(SoundFilePathShort,                                        pattern = '.wav', n = 2)[,1]#
  # Count the number of slashes in the first file path  nslash <- str_count(SoundFilePathShort[1], '/') + 1#
  # Split the file paths based on slashes and keep the last part (file name)  SoundFilePathShort <- str_split_fixed(SoundFilePathShort,                                        pattern = '/', n = nslash)[, nslash]#
  # Extract the times from the file names  times <- as.numeric(substr(str_split_fixed(SoundFilePathShort, pattern = '_', n = 3)[, 3], 1, 2))#
  # Select the indices of times that are between 6 and 10   times.index <- which(times == 5 | times == 6 | times == 7 | times == 7 )#
  # Filter the full file paths and file names based on the selected indices  SoundFilePathFull <- SoundFilePathFull[times.index]  SoundFilePathShort <- SoundFilePathShort[times.index]#
  # Set parameters for processing sound clips  clip.duration <- 12       # Duration of each sound clip  hop.size <- 6             # Hop size for splitting the sound clips  downsample.rate <- 'NA'   # Downsample rate for audio in Hz, otherwise NA  threshold <- 0.5         # Threshold for audio detection  sav.wav <- T              # Save the extracted sound clips as WAV files?#
  # Create output folders if they don't exist  dir.create(OutputFolder, recursive = TRUE, showWarnings = FALSE)  dir.create(OutputFolderSelections, recursive = TRUE, showWarnings = FALSE)  dir.create(OutputFolderWav, recursive = TRUE, showWarnings = FALSE)#
  #which(str_detect( SoundFilePathShort,'R1053_20220511_060002'))  for(x in (797:length(SoundFilePathFull)) ){ tryCatch({    RavenSelectionTableDFAlexNet <- data.frame()#
    start.time.detection <- Sys.time()    print(paste(x, 'out of', length(SoundFilePathFull)))    TempWav <- readWave(SoundFilePathFull[x])    WavDur <- duration(TempWav)#
    Seq.start <- list()    Seq.end <- list()#
    i <- 1    while (i + clip.duration < WavDur) {      # print(i)      Seq.start[[i]] = i      Seq.end[[i]] = i+clip.duration      i= i+hop.size    }#
    ClipStart <- unlist(Seq.start)    ClipEnd <- unlist(Seq.end)#
    TempClips <- cbind.data.frame(ClipStart,ClipEnd)#
    # Subset sound clips for classification -----------------------------------    print('saving sound clips')    set.seed(13)    length <- nrow(TempClips)#
    if(length > 100){    length.files <- seq(1,length,100)    } else {      length.files <- c(1,length)    }#
    for(q in 1: (length(length.files)-1) ){      unlink('/Users/denaclink/Desktop/datacopy/Temp/WavFiles', recursive = TRUE)      unlink('/Users/denaclink/Desktop/datacopy/Temp/Images/Images', recursive = TRUE)#
      dir.create('/Users/denaclink/Desktop/datacopy/Temp/WavFiles')      dir.create('/Users/denaclink/Desktop/datacopy/Temp/Images/Images')#
      RandomSub <-  seq(length.files[q],length.files[q+1],1)#
      if(q== (length(length.files)-1) ){        RandomSub <-  seq(length.files[q],length,1)      }#
      start.time <- TempClips$ClipStart[RandomSub]      end.time <- TempClips$ClipEnd[RandomSub]#
      short.sound.files <- lapply(1:length(start.time),                                  function(i)                                    extractWave(                                      TempWav,                                      from = start.time[i],                                      to = end.time[i],                                      xunit = c("time"),                                      plot = F,                                      output = "Wave"                                    ))#
      if(downsample.rate != 'NA'){      print('downsampling')      short.sound.files <- lapply(1:length(short.sound.files),                                  function(i)                                    downsample(                                      short.sound.files[[i]],                                      samp.rate=downsample.rate                                    ))      }#
      for(d in 1:length(short.sound.files)){        #print(d)        writeWave(short.sound.files[[d]],paste('/Users/denaclink/Desktop/datacopy/Temp/WavFiles','/',                                               SoundFilePathShort[x],'_',start.time[d], '.wav', sep=''),                  extensible = F)      }#
      # Save images to a temp folder      print(paste('Creating images',start.time[1],'start time clips'))#
      for(e in 1:length(short.sound.files)){        jpeg(paste('/Users/denaclink/Desktop/datacopy/Temp/Images/Images','/', SoundFilePathShort[x],'_',start.time[e],'.jpg',sep=''),res = 50)        short.wav <- short.sound.files[[e]]#
        seewave::spectro(short.wav,tlab='',flab='',axisX=F,axisY = F,scale=F,grid=F,flim=c(0.4,3),fastdisp=TRUE,noisereduction=1)#
        graphics.off()      }#
      # Predict using AlexNet ----------------------------------------------------      print('Classifying images using AlexNet')#
     test.input <- '/Users/denaclink/Desktop/datacopy/Temp/Images/'#
     # ResNet     test_ds <- image_folder_dataset(       file.path(test.input),       transform = . %>%         torchvision::transform_to_tensor() %>%         torchvision::transform_resize(size = c(224, 224)) %>%         torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),       target_transform = function(x) as.double(x) - 1)#
     # AlexNet and VGG19     # test_ds <- image_folder_dataset(     #    file.path(test.input ),     #    transform = . %>%     #      torchvision::transform_to_tensor() %>%     #      torchvision::transform_color_jitter() %>%     #      transform_resize(256) %>%     #      transform_center_crop(224) %>%     #      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )     #  #
      # Predict the test files      # Variable indicating the number of files      nfiles <- test_ds$.length()#
      # Load the test images      test_dl <- dataloader(test_ds, batch_size =nfiles)#
      # Predict using AlexNet      AlexNetPred <- predict(modelAlexNetGibbon, test_dl)      AlexNetProb <- torch_sigmoid(AlexNetPred)      AlexNetProb <- 1-as_array(torch_tensor(AlexNetProb,device = 'cpu'))      AlexNetClass <- ifelse((AlexNetProb) >= threshold, "Gibbon", "noise")#
      # Calculate the probability associated with each class      Probability <- AlexNetProb#
      image.files <- list.files(file.path(test.input),recursive = T,                                full.names = T)      nslash <- str_count(image.files,'/')+1      nslash <- nslash[1]      image.files.short <- str_split_fixed(image.files,pattern = '/',n=nslash)[,nslash]      image.files.short <- str_split_fixed(image.files.short,pattern = '.jpg',n=2)[,1]#
      print('Saving output')      Detections <-  which(Probability >= threshold )#
      Detections <-  split(Detections, cumsum(c(        1, diff(Detections)) != 1))#
      for(i in 1:length(Detections)){        TempList <- Detections[[i]]        if(length(TempList)==1){          Detections[[i]] <- TempList[1]        }        if(length(TempList)==2){          Detections[[i]] <- TempList[2]        }        if(length(TempList)> 2){          Detections[[i]] <- median(TempList)        }#
      }#
      DetectionIndices <- unname(unlist(Detections))#
      print('Saving output')      file.copy(image.files[DetectionIndices],                to= paste(OutputFolder,                          image.files.short[DetectionIndices],                          '_',                          round(Probability[DetectionIndices],2),                          '_AlexNet_.jpg', sep=''))#
      if(sav.wav ==T){        wav.file.paths <- list.files('/Users/denaclink/Desktop/datacopy/Temp/WavFiles',full.names = T)        file.copy(wav.file.paths[DetectionIndices],                  to= paste(OutputFolderWav,                            image.files.short[DetectionIndices],                            '_',                            round(Probability[DetectionIndices],2),                            '_AlexNet_.wav', sep=''))      }#
      Detections <- image.files.short[DetectionIndices]#
      if (length(Detections) > 0) {        Selection <- seq(1, length(Detections))        View <- rep('Spectrogram 1', length(Detections))        Channel <- rep(1, length(Detections))        MinFreq <- rep(100, length(Detections))        MaxFreq <- rep(2000, length(Detections))        start.time.new <- as.numeric(str_split_fixed(Detections,pattern = '_',n=4)[,4])        end.time.new <- start.time.new + clip.duration        Probability <- round(Probability[DetectionIndices],2)#
        RavenSelectionTableDFAlexNetTemp <-          cbind.data.frame(Selection,                           View,                           Channel,                           MinFreq,                           MaxFreq,start.time.new,end.time.new,Probability,                           Detections)#
        RavenSelectionTableDFAlexNetTemp <-          RavenSelectionTableDFAlexNetTemp[, c(            "Selection",            "View",            "Channel",            "start.time.new",            "end.time.new",            "MinFreq",            "MaxFreq",            'Probability',"Detections"          )]#
        colnames(RavenSelectionTableDFAlexNetTemp) <-          c(            "Selection",            "View",            "Channel",            "Begin Time (s)",            "End Time (s)",            "Low Freq (Hz)",            "High Freq (Hz)",            'Probability',            "Detections"          )#
        RavenSelectionTableDFAlexNet <- rbind.data.frame(RavenSelectionTableDFAlexNet,                                                       RavenSelectionTableDFAlexNetTemp)#
        if(nrow(RavenSelectionTableDFAlexNet) > 0){          csv.file.name <-            paste(OutputFolderSelections,                  SoundFilePathShort[x],                  'GibbonAlexNetAllFilesMalaysia.txt',                  sep = '')#
          write.table(            x = RavenSelectionTableDFAlexNet,            sep = "\t",            file = csv.file.name,            row.names = FALSE,            quote = FALSE          )          print(paste(            "Saving Selection Table"          ))        }#
      }    }#
    if(nrow(RavenSelectionTableDFAlexNet) == 0){      csv.file.name <-        paste(OutputFolderSelections,              SoundFilePathShort[x],              'GibbonAlexNetAllFilesMalaysia.txt',              sep = '')#
     ColNames <-  c(       "Selection",       "View",       "Channel",       "Begin Time (s)",       "End Time (s)",       "Low Freq (Hz)",       "High Freq (Hz)",       'Probability',       "Detections"     )#
      TempNARow <- t(as.data.frame(rep(NA,length(ColNames))))#
      colnames(TempNARow) <- ColNames#
      write.table(        x = TempNARow,        sep = "\t",        file = csv.file.name,        row.names = FALSE,        quote = FALSE      )      print(paste(        "Saving Selection Table"      ))    }    rm(TempWav)    rm(short.sound.files)    rm( test_ds )    rm(short.wav)    end.time.detection <- Sys.time()    print(end.time.detection-start.time.detection)    gc()  }, error = function(e) { cat("ERROR :", conditionMessage(e), "\n") })  }
for(x in (800:length(SoundFilePathFull)) ){ tryCatch({    RavenSelectionTableDFAlexNet <- data.frame()#
    start.time.detection <- Sys.time()    print(paste(x, 'out of', length(SoundFilePathFull)))    TempWav <- readWave(SoundFilePathFull[x])    WavDur <- duration(TempWav)#
    Seq.start <- list()    Seq.end <- list()#
    i <- 1    while (i + clip.duration < WavDur) {      # print(i)      Seq.start[[i]] = i      Seq.end[[i]] = i+clip.duration      i= i+hop.size    }#
    ClipStart <- unlist(Seq.start)    ClipEnd <- unlist(Seq.end)#
    TempClips <- cbind.data.frame(ClipStart,ClipEnd)#
    # Subset sound clips for classification -----------------------------------    print('saving sound clips')    set.seed(13)    length <- nrow(TempClips)#
    if(length > 100){    length.files <- seq(1,length,100)    } else {      length.files <- c(1,length)    }#
    for(q in 1: (length(length.files)-1) ){      unlink('/Users/denaclink/Desktop/datacopy/Temp/WavFiles', recursive = TRUE)      unlink('/Users/denaclink/Desktop/datacopy/Temp/Images/Images', recursive = TRUE)#
      dir.create('/Users/denaclink/Desktop/datacopy/Temp/WavFiles')      dir.create('/Users/denaclink/Desktop/datacopy/Temp/Images/Images')#
      RandomSub <-  seq(length.files[q],length.files[q+1],1)#
      if(q== (length(length.files)-1) ){        RandomSub <-  seq(length.files[q],length,1)      }#
      start.time <- TempClips$ClipStart[RandomSub]      end.time <- TempClips$ClipEnd[RandomSub]#
      short.sound.files <- lapply(1:length(start.time),                                  function(i)                                    extractWave(                                      TempWav,                                      from = start.time[i],                                      to = end.time[i],                                      xunit = c("time"),                                      plot = F,                                      output = "Wave"                                    ))#
      if(downsample.rate != 'NA'){      print('downsampling')      short.sound.files <- lapply(1:length(short.sound.files),                                  function(i)                                    downsample(                                      short.sound.files[[i]],                                      samp.rate=downsample.rate                                    ))      }#
      for(d in 1:length(short.sound.files)){        #print(d)        writeWave(short.sound.files[[d]],paste('/Users/denaclink/Desktop/datacopy/Temp/WavFiles','/',                                               SoundFilePathShort[x],'_',start.time[d], '.wav', sep=''),                  extensible = F)      }#
      # Save images to a temp folder      print(paste('Creating images',start.time[1],'start time clips'))#
      for(e in 1:length(short.sound.files)){        jpeg(paste('/Users/denaclink/Desktop/datacopy/Temp/Images/Images','/', SoundFilePathShort[x],'_',start.time[e],'.jpg',sep=''),res = 50)        short.wav <- short.sound.files[[e]]#
        seewave::spectro(short.wav,tlab='',flab='',axisX=F,axisY = F,scale=F,grid=F,flim=c(0.4,3),fastdisp=TRUE,noisereduction=1)#
        graphics.off()      }#
      # Predict using AlexNet ----------------------------------------------------      print('Classifying images using AlexNet')#
     test.input <- '/Users/denaclink/Desktop/datacopy/Temp/Images/'#
     # ResNet     test_ds <- image_folder_dataset(       file.path(test.input),       transform = . %>%         torchvision::transform_to_tensor() %>%         torchvision::transform_resize(size = c(224, 224)) %>%         torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),       target_transform = function(x) as.double(x) - 1)#
     # AlexNet and VGG19     # test_ds <- image_folder_dataset(     #    file.path(test.input ),     #    transform = . %>%     #      torchvision::transform_to_tensor() %>%     #      torchvision::transform_color_jitter() %>%     #      transform_resize(256) %>%     #      transform_center_crop(224) %>%     #      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )     #  #
      # Predict the test files      # Variable indicating the number of files      nfiles <- test_ds$.length()#
      # Load the test images      test_dl <- dataloader(test_ds, batch_size =nfiles)#
      # Predict using AlexNet      AlexNetPred <- predict(modelAlexNetGibbon, test_dl)      AlexNetProb <- torch_sigmoid(AlexNetPred)      AlexNetProb <- 1-as_array(torch_tensor(AlexNetProb,device = 'cpu'))      AlexNetClass <- ifelse((AlexNetProb) >= threshold, "Gibbon", "noise")#
      # Calculate the probability associated with each class      Probability <- AlexNetProb#
      image.files <- list.files(file.path(test.input),recursive = T,                                full.names = T)      nslash <- str_count(image.files,'/')+1      nslash <- nslash[1]      image.files.short <- str_split_fixed(image.files,pattern = '/',n=nslash)[,nslash]      image.files.short <- str_split_fixed(image.files.short,pattern = '.jpg',n=2)[,1]#
      print('Saving output')      Detections <-  which(Probability >= threshold )#
      Detections <-  split(Detections, cumsum(c(        1, diff(Detections)) != 1))#
      for(i in 1:length(Detections)){        TempList <- Detections[[i]]        if(length(TempList)==1){          Detections[[i]] <- TempList[1]        }        if(length(TempList)==2){          Detections[[i]] <- TempList[2]        }        if(length(TempList)> 2){          Detections[[i]] <- median(TempList)        }#
      }#
      DetectionIndices <- unname(unlist(Detections))#
      print('Saving output')      file.copy(image.files[DetectionIndices],                to= paste(OutputFolder,                          image.files.short[DetectionIndices],                          '_',                          round(Probability[DetectionIndices],2),                          '_AlexNet_.jpg', sep=''))#
      if(sav.wav ==T){        wav.file.paths <- list.files('/Users/denaclink/Desktop/datacopy/Temp/WavFiles',full.names = T)        file.copy(wav.file.paths[DetectionIndices],                  to= paste(OutputFolderWav,                            image.files.short[DetectionIndices],                            '_',                            round(Probability[DetectionIndices],2),                            '_AlexNet_.wav', sep=''))      }#
      Detections <- image.files.short[DetectionIndices]#
      if (length(Detections) > 0) {        Selection <- seq(1, length(Detections))        View <- rep('Spectrogram 1', length(Detections))        Channel <- rep(1, length(Detections))        MinFreq <- rep(100, length(Detections))        MaxFreq <- rep(2000, length(Detections))        start.time.new <- as.numeric(str_split_fixed(Detections,pattern = '_',n=4)[,4])        end.time.new <- start.time.new + clip.duration        Probability <- round(Probability[DetectionIndices],2)#
        RavenSelectionTableDFAlexNetTemp <-          cbind.data.frame(Selection,                           View,                           Channel,                           MinFreq,                           MaxFreq,start.time.new,end.time.new,Probability,                           Detections)#
        RavenSelectionTableDFAlexNetTemp <-          RavenSelectionTableDFAlexNetTemp[, c(            "Selection",            "View",            "Channel",            "start.time.new",            "end.time.new",            "MinFreq",            "MaxFreq",            'Probability',"Detections"          )]#
        colnames(RavenSelectionTableDFAlexNetTemp) <-          c(            "Selection",            "View",            "Channel",            "Begin Time (s)",            "End Time (s)",            "Low Freq (Hz)",            "High Freq (Hz)",            'Probability',            "Detections"          )#
        RavenSelectionTableDFAlexNet <- rbind.data.frame(RavenSelectionTableDFAlexNet,                                                       RavenSelectionTableDFAlexNetTemp)#
        if(nrow(RavenSelectionTableDFAlexNet) > 0){          csv.file.name <-            paste(OutputFolderSelections,                  SoundFilePathShort[x],                  'GibbonAlexNetAllFilesMalaysia.txt',                  sep = '')#
          write.table(            x = RavenSelectionTableDFAlexNet,            sep = "\t",            file = csv.file.name,            row.names = FALSE,            quote = FALSE          )          print(paste(            "Saving Selection Table"          ))        }#
      }    }#
    if(nrow(RavenSelectionTableDFAlexNet) == 0){      csv.file.name <-        paste(OutputFolderSelections,              SoundFilePathShort[x],              'GibbonAlexNetAllFilesMalaysia.txt',              sep = '')#
     ColNames <-  c(       "Selection",       "View",       "Channel",       "Begin Time (s)",       "End Time (s)",       "Low Freq (Hz)",       "High Freq (Hz)",       'Probability',       "Detections"     )#
      TempNARow <- t(as.data.frame(rep(NA,length(ColNames))))#
      colnames(TempNARow) <- ColNames#
      write.table(        x = TempNARow,        sep = "\t",        file = csv.file.name,        row.names = FALSE,        quote = FALSE      )      print(paste(        "Saving Selection Table"      ))    }    rm(TempWav)    rm(short.sound.files)    rm( test_ds )    rm(short.wav)    end.time.detection <- Sys.time()    print(end.time.detection-start.time.detection)    gc()  }, error = function(e) { cat("ERROR :", conditionMessage(e), "\n") })  }
#which(str_detect( SoundFilePathShort,'R1053_20220512_05000'))  for(x in (1261:length(SoundFilePathFull)) ){ tryCatch({    RavenSelectionTableDFAlexNet <- data.frame()#
    start.time.detection <- Sys.time()    print(paste(x, 'out of', length(SoundFilePathFull)))    TempWav <- readWave(SoundFilePathFull[x])    WavDur <- duration(TempWav)#
    Seq.start <- list()    Seq.end <- list()#
    i <- 1    while (i + clip.duration < WavDur) {      # print(i)      Seq.start[[i]] = i      Seq.end[[i]] = i+clip.duration      i= i+hop.size    }#
    ClipStart <- unlist(Seq.start)    ClipEnd <- unlist(Seq.end)#
    TempClips <- cbind.data.frame(ClipStart,ClipEnd)#
    # Subset sound clips for classification -----------------------------------    print('saving sound clips')    set.seed(13)    length <- nrow(TempClips)#
    if(length > 100){    length.files <- seq(1,length,100)    } else {      length.files <- c(1,length)    }#
    for(q in 1: (length(length.files)-1) ){      unlink('/Users/denaclink/Desktop/datacopy/Temp/WavFiles', recursive = TRUE)      unlink('/Users/denaclink/Desktop/datacopy/Temp/Images/Images', recursive = TRUE)#
      dir.create('/Users/denaclink/Desktop/datacopy/Temp/WavFiles')      dir.create('/Users/denaclink/Desktop/datacopy/Temp/Images/Images')#
      RandomSub <-  seq(length.files[q],length.files[q+1],1)#
      if(q== (length(length.files)-1) ){        RandomSub <-  seq(length.files[q],length,1)      }#
      start.time <- TempClips$ClipStart[RandomSub]      end.time <- TempClips$ClipEnd[RandomSub]#
      short.sound.files <- lapply(1:length(start.time),                                  function(i)                                    extractWave(                                      TempWav,                                      from = start.time[i],                                      to = end.time[i],                                      xunit = c("time"),                                      plot = F,                                      output = "Wave"                                    ))#
      if(downsample.rate != 'NA'){      print('downsampling')      short.sound.files <- lapply(1:length(short.sound.files),                                  function(i)                                    downsample(                                      short.sound.files[[i]],                                      samp.rate=downsample.rate                                    ))      }#
      for(d in 1:length(short.sound.files)){        #print(d)        writeWave(short.sound.files[[d]],paste('/Users/denaclink/Desktop/datacopy/Temp/WavFiles','/',                                               SoundFilePathShort[x],'_',start.time[d], '.wav', sep=''),                  extensible = F)      }#
      # Save images to a temp folder      print(paste('Creating images',start.time[1],'start time clips'))#
      for(e in 1:length(short.sound.files)){        jpeg(paste('/Users/denaclink/Desktop/datacopy/Temp/Images/Images','/', SoundFilePathShort[x],'_',start.time[e],'.jpg',sep=''),res = 50)        short.wav <- short.sound.files[[e]]#
        seewave::spectro(short.wav,tlab='',flab='',axisX=F,axisY = F,scale=F,grid=F,flim=c(0.4,3),fastdisp=TRUE,noisereduction=1)#
        graphics.off()      }#
      # Predict using AlexNet ----------------------------------------------------      print('Classifying images using AlexNet')#
     test.input <- '/Users/denaclink/Desktop/datacopy/Temp/Images/'#
     # ResNet     test_ds <- image_folder_dataset(       file.path(test.input),       transform = . %>%         torchvision::transform_to_tensor() %>%         torchvision::transform_resize(size = c(224, 224)) %>%         torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),       target_transform = function(x) as.double(x) - 1)#
     # AlexNet and VGG19     # test_ds <- image_folder_dataset(     #    file.path(test.input ),     #    transform = . %>%     #      torchvision::transform_to_tensor() %>%     #      torchvision::transform_color_jitter() %>%     #      transform_resize(256) %>%     #      transform_center_crop(224) %>%     #      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )     #  #
      # Predict the test files      # Variable indicating the number of files      nfiles <- test_ds$.length()#
      # Load the test images      test_dl <- dataloader(test_ds, batch_size =nfiles)#
      # Predict using AlexNet      AlexNetPred <- predict(modelAlexNetGibbon, test_dl)      AlexNetProb <- torch_sigmoid(AlexNetPred)      AlexNetProb <- 1-as_array(torch_tensor(AlexNetProb,device = 'cpu'))      AlexNetClass <- ifelse((AlexNetProb) >= threshold, "Gibbon", "noise")#
      # Calculate the probability associated with each class      Probability <- AlexNetProb#
      image.files <- list.files(file.path(test.input),recursive = T,                                full.names = T)      nslash <- str_count(image.files,'/')+1      nslash <- nslash[1]      image.files.short <- str_split_fixed(image.files,pattern = '/',n=nslash)[,nslash]      image.files.short <- str_split_fixed(image.files.short,pattern = '.jpg',n=2)[,1]#
      print('Saving output')      Detections <-  which(Probability >= threshold )#
      Detections <-  split(Detections, cumsum(c(        1, diff(Detections)) != 1))#
      for(i in 1:length(Detections)){        TempList <- Detections[[i]]        if(length(TempList)==1){          Detections[[i]] <- TempList[1]        }        if(length(TempList)==2){          Detections[[i]] <- TempList[2]        }        if(length(TempList)> 2){          Detections[[i]] <- median(TempList)        }#
      }#
      DetectionIndices <- unname(unlist(Detections))#
      print('Saving output')      file.copy(image.files[DetectionIndices],                to= paste(OutputFolder,                          image.files.short[DetectionIndices],                          '_',                          round(Probability[DetectionIndices],2),                          '_AlexNet_.jpg', sep=''))#
      if(sav.wav ==T){        wav.file.paths <- list.files('/Users/denaclink/Desktop/datacopy/Temp/WavFiles',full.names = T)        file.copy(wav.file.paths[DetectionIndices],                  to= paste(OutputFolderWav,                            image.files.short[DetectionIndices],                            '_',                            round(Probability[DetectionIndices],2),                            '_AlexNet_.wav', sep=''))      }#
      Detections <- image.files.short[DetectionIndices]#
      if (length(Detections) > 0) {        Selection <- seq(1, length(Detections))        View <- rep('Spectrogram 1', length(Detections))        Channel <- rep(1, length(Detections))        MinFreq <- rep(100, length(Detections))        MaxFreq <- rep(2000, length(Detections))        start.time.new <- as.numeric(str_split_fixed(Detections,pattern = '_',n=4)[,4])        end.time.new <- start.time.new + clip.duration        Probability <- round(Probability[DetectionIndices],2)#
        RavenSelectionTableDFAlexNetTemp <-          cbind.data.frame(Selection,                           View,                           Channel,                           MinFreq,                           MaxFreq,start.time.new,end.time.new,Probability,                           Detections)#
        RavenSelectionTableDFAlexNetTemp <-          RavenSelectionTableDFAlexNetTemp[, c(            "Selection",            "View",            "Channel",            "start.time.new",            "end.time.new",            "MinFreq",            "MaxFreq",            'Probability',"Detections"          )]#
        colnames(RavenSelectionTableDFAlexNetTemp) <-          c(            "Selection",            "View",            "Channel",            "Begin Time (s)",            "End Time (s)",            "Low Freq (Hz)",            "High Freq (Hz)",            'Probability',            "Detections"          )#
        RavenSelectionTableDFAlexNet <- rbind.data.frame(RavenSelectionTableDFAlexNet,                                                       RavenSelectionTableDFAlexNetTemp)#
        if(nrow(RavenSelectionTableDFAlexNet) > 0){          csv.file.name <-            paste(OutputFolderSelections,                  SoundFilePathShort[x],                  'GibbonAlexNetAllFilesMalaysia.txt',                  sep = '')#
          write.table(            x = RavenSelectionTableDFAlexNet,            sep = "\t",            file = csv.file.name,            row.names = FALSE,            quote = FALSE          )          print(paste(            "Saving Selection Table"          ))        }#
      }    }#
    if(nrow(RavenSelectionTableDFAlexNet) == 0){      csv.file.name <-        paste(OutputFolderSelections,              SoundFilePathShort[x],              'GibbonAlexNetAllFilesMalaysia.txt',              sep = '')#
     ColNames <-  c(       "Selection",       "View",       "Channel",       "Begin Time (s)",       "End Time (s)",       "Low Freq (Hz)",       "High Freq (Hz)",       'Probability',       "Detections"     )#
      TempNARow <- t(as.data.frame(rep(NA,length(ColNames))))#
      colnames(TempNARow) <- ColNames#
      write.table(        x = TempNARow,        sep = "\t",        file = csv.file.name,        row.names = FALSE,        quote = FALSE      )      print(paste(        "Saving Selection Table"      ))    }    rm(TempWav)    rm(short.sound.files)    rm( test_ds )    rm(short.wav)    end.time.detection <- Sys.time()    print(end.time.detection-start.time.detection)    gc()  }, error = function(e) { cat("ERROR :", conditionMessage(e), "\n") })  }
path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM 15_12_22'  # Already run above this line ##
  #   # path.to.files <-'/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM_04_03_23'  # Doesn't have all 10 #/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM_08_07_23'#
  # Get a list of full file paths of sound files in a directory  SoundFilePathFull <- list.files(path.to.files,                                  recursive = T, pattern = '.wav', full.names = T)#
  # Get a list of file names of sound files in a directory  SoundFilePathShort <- list.files(path.to.files,                                   recursive = T, pattern = '.wav')#
  # Extract only the file names without the extension  SoundFilePathShort <- str_split_fixed(SoundFilePathShort,                                        pattern = '.wav', n = 2)[,1]#
  # Count the number of slashes in the first file path  nslash <- str_count(SoundFilePathShort[1], '/') + 1#
  # Split the file paths based on slashes and keep the last part (file name)  SoundFilePathShort <- str_split_fixed(SoundFilePathShort,                                        pattern = '/', n = nslash)[, nslash]#
  # Extract the times from the file names  times <- as.numeric(substr(str_split_fixed(SoundFilePathShort, pattern = '_', n = 3)[, 3], 1, 2))#
  # Select the indices of times that are between 6 and 10   times.index <- which(times == 5 | times == 6 | times == 7 | times == 7 )#
  # Filter the full file paths and file names based on the selected indices  SoundFilePathFull <- SoundFilePathFull[times.index]  SoundFilePathShort <- SoundFilePathShort[times.index]#
  # Set parameters for processing sound clips  clip.duration <- 12       # Duration of each sound clip  hop.size <- 6             # Hop size for splitting the sound clips  downsample.rate <- 'NA'   # Downsample rate for audio in Hz, otherwise NA  threshold <- 0.5         # Threshold for audio detection  sav.wav <- T              # Save the extracted sound clips as WAV files?#
  # Create output folders if they don't exist  dir.create(OutputFolder, recursive = TRUE, showWarnings = FALSE)  dir.create(OutputFolderSelections, recursive = TRUE, showWarnings = FALSE)  dir.create(OutputFolderWav, recursive = TRUE, showWarnings = FALSE)#
  #which(str_detect( SoundFilePathShort,'R1053_20220512_05000'))  for(x in (1:length(SoundFilePathFull)) ){ tryCatch({    RavenSelectionTableDFAlexNet <- data.frame()#
    start.time.detection <- Sys.time()    print(paste(x, 'out of', length(SoundFilePathFull)))    TempWav <- readWave(SoundFilePathFull[x])    WavDur <- duration(TempWav)#
    Seq.start <- list()    Seq.end <- list()#
    i <- 1    while (i + clip.duration < WavDur) {      # print(i)      Seq.start[[i]] = i      Seq.end[[i]] = i+clip.duration      i= i+hop.size    }#
    ClipStart <- unlist(Seq.start)    ClipEnd <- unlist(Seq.end)#
    TempClips <- cbind.data.frame(ClipStart,ClipEnd)#
    # Subset sound clips for classification -----------------------------------    print('saving sound clips')    set.seed(13)    length <- nrow(TempClips)#
    if(length > 100){    length.files <- seq(1,length,100)    } else {      length.files <- c(1,length)    }#
    for(q in 1: (length(length.files)-1) ){      unlink('/Users/denaclink/Desktop/datacopy/Temp/WavFiles', recursive = TRUE)      unlink('/Users/denaclink/Desktop/datacopy/Temp/Images/Images', recursive = TRUE)#
      dir.create('/Users/denaclink/Desktop/datacopy/Temp/WavFiles')      dir.create('/Users/denaclink/Desktop/datacopy/Temp/Images/Images')#
      RandomSub <-  seq(length.files[q],length.files[q+1],1)#
      if(q== (length(length.files)-1) ){        RandomSub <-  seq(length.files[q],length,1)      }#
      start.time <- TempClips$ClipStart[RandomSub]      end.time <- TempClips$ClipEnd[RandomSub]#
      short.sound.files <- lapply(1:length(start.time),                                  function(i)                                    extractWave(                                      TempWav,                                      from = start.time[i],                                      to = end.time[i],                                      xunit = c("time"),                                      plot = F,                                      output = "Wave"                                    ))#
      if(downsample.rate != 'NA'){      print('downsampling')      short.sound.files <- lapply(1:length(short.sound.files),                                  function(i)                                    downsample(                                      short.sound.files[[i]],                                      samp.rate=downsample.rate                                    ))      }#
      for(d in 1:length(short.sound.files)){        #print(d)        writeWave(short.sound.files[[d]],paste('/Users/denaclink/Desktop/datacopy/Temp/WavFiles','/',                                               SoundFilePathShort[x],'_',start.time[d], '.wav', sep=''),                  extensible = F)      }#
      # Save images to a temp folder      print(paste('Creating images',start.time[1],'start time clips'))#
      for(e in 1:length(short.sound.files)){        jpeg(paste('/Users/denaclink/Desktop/datacopy/Temp/Images/Images','/', SoundFilePathShort[x],'_',start.time[e],'.jpg',sep=''),res = 50)        short.wav <- short.sound.files[[e]]#
        seewave::spectro(short.wav,tlab='',flab='',axisX=F,axisY = F,scale=F,grid=F,flim=c(0.4,3),fastdisp=TRUE,noisereduction=1)#
        graphics.off()      }#
      # Predict using AlexNet ----------------------------------------------------      print('Classifying images using AlexNet')#
     test.input <- '/Users/denaclink/Desktop/datacopy/Temp/Images/'#
     # ResNet     test_ds <- image_folder_dataset(       file.path(test.input),       transform = . %>%         torchvision::transform_to_tensor() %>%         torchvision::transform_resize(size = c(224, 224)) %>%         torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),       target_transform = function(x) as.double(x) - 1)#
     # AlexNet and VGG19     # test_ds <- image_folder_dataset(     #    file.path(test.input ),     #    transform = . %>%     #      torchvision::transform_to_tensor() %>%     #      torchvision::transform_color_jitter() %>%     #      transform_resize(256) %>%     #      transform_center_crop(224) %>%     #      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )     #  #
      # Predict the test files      # Variable indicating the number of files      nfiles <- test_ds$.length()#
      # Load the test images      test_dl <- dataloader(test_ds, batch_size =nfiles)#
      # Predict using AlexNet      AlexNetPred <- predict(modelAlexNetGibbon, test_dl)      AlexNetProb <- torch_sigmoid(AlexNetPred)      AlexNetProb <- 1-as_array(torch_tensor(AlexNetProb,device = 'cpu'))      AlexNetClass <- ifelse((AlexNetProb) >= threshold, "Gibbon", "noise")#
      # Calculate the probability associated with each class      Probability <- AlexNetProb#
      image.files <- list.files(file.path(test.input),recursive = T,                                full.names = T)      nslash <- str_count(image.files,'/')+1      nslash <- nslash[1]      image.files.short <- str_split_fixed(image.files,pattern = '/',n=nslash)[,nslash]      image.files.short <- str_split_fixed(image.files.short,pattern = '.jpg',n=2)[,1]#
      print('Saving output')      Detections <-  which(Probability >= threshold )#
      Detections <-  split(Detections, cumsum(c(        1, diff(Detections)) != 1))#
      for(i in 1:length(Detections)){        TempList <- Detections[[i]]        if(length(TempList)==1){          Detections[[i]] <- TempList[1]        }        if(length(TempList)==2){          Detections[[i]] <- TempList[2]        }        if(length(TempList)> 2){          Detections[[i]] <- median(TempList)        }#
      }#
      DetectionIndices <- unname(unlist(Detections))#
      print('Saving output')      file.copy(image.files[DetectionIndices],                to= paste(OutputFolder,                          image.files.short[DetectionIndices],                          '_',                          round(Probability[DetectionIndices],2),                          '_AlexNet_.jpg', sep=''))#
      if(sav.wav ==T){        wav.file.paths <- list.files('/Users/denaclink/Desktop/datacopy/Temp/WavFiles',full.names = T)        file.copy(wav.file.paths[DetectionIndices],                  to= paste(OutputFolderWav,                            image.files.short[DetectionIndices],                            '_',                            round(Probability[DetectionIndices],2),                            '_AlexNet_.wav', sep=''))      }#
      Detections <- image.files.short[DetectionIndices]#
      if (length(Detections) > 0) {        Selection <- seq(1, length(Detections))        View <- rep('Spectrogram 1', length(Detections))        Channel <- rep(1, length(Detections))        MinFreq <- rep(100, length(Detections))        MaxFreq <- rep(2000, length(Detections))        start.time.new <- as.numeric(str_split_fixed(Detections,pattern = '_',n=4)[,4])        end.time.new <- start.time.new + clip.duration        Probability <- round(Probability[DetectionIndices],2)#
        RavenSelectionTableDFAlexNetTemp <-          cbind.data.frame(Selection,                           View,                           Channel,                           MinFreq,                           MaxFreq,start.time.new,end.time.new,Probability,                           Detections)#
        RavenSelectionTableDFAlexNetTemp <-          RavenSelectionTableDFAlexNetTemp[, c(            "Selection",            "View",            "Channel",            "start.time.new",            "end.time.new",            "MinFreq",            "MaxFreq",            'Probability',"Detections"          )]#
        colnames(RavenSelectionTableDFAlexNetTemp) <-          c(            "Selection",            "View",            "Channel",            "Begin Time (s)",            "End Time (s)",            "Low Freq (Hz)",            "High Freq (Hz)",            'Probability',            "Detections"          )#
        RavenSelectionTableDFAlexNet <- rbind.data.frame(RavenSelectionTableDFAlexNet,                                                       RavenSelectionTableDFAlexNetTemp)#
        if(nrow(RavenSelectionTableDFAlexNet) > 0){          csv.file.name <-            paste(OutputFolderSelections,                  SoundFilePathShort[x],                  'GibbonAlexNetAllFilesMalaysia.txt',                  sep = '')#
          write.table(            x = RavenSelectionTableDFAlexNet,            sep = "\t",            file = csv.file.name,            row.names = FALSE,            quote = FALSE          )          print(paste(            "Saving Selection Table"          ))        }#
      }    }#
    if(nrow(RavenSelectionTableDFAlexNet) == 0){      csv.file.name <-        paste(OutputFolderSelections,              SoundFilePathShort[x],              'GibbonAlexNetAllFilesMalaysia.txt',              sep = '')#
     ColNames <-  c(       "Selection",       "View",       "Channel",       "Begin Time (s)",       "End Time (s)",       "Low Freq (Hz)",       "High Freq (Hz)",       'Probability',       "Detections"     )#
      TempNARow <- t(as.data.frame(rep(NA,length(ColNames))))#
      colnames(TempNARow) <- ColNames#
      write.table(        x = TempNARow,        sep = "\t",        file = csv.file.name,        row.names = FALSE,        quote = FALSE      )      print(paste(        "Saving Selection Table"      ))    }    rm(TempWav)    rm(short.sound.files)    rm( test_ds )    rm(short.wav)    end.time.detection <- Sys.time()    print(end.time.detection-start.time.detection)    gc()  }, error = function(e) { cat("ERROR :", conditionMessage(e), "\n") })  }
# Prepare data ------------------------------------------------------------#
  # Load required libraries  library(luz)                # Luz library for deep learning with PyTorch  library(torch)              # PyTorch library  library(torchvision)        # PyTorch library for computer vision tasks  library(torchdatasets)      # PyTorch library for handling datasets#
  library(stringr)            # String manipulation library  library(tuneR)              # Library for audio analysis  library(seewave)            # Library for sound analysis  #library(gibbonR)            # Library for bioacoustic analysis#
  # NEED TO ADD:  metadata output#
  # Set the output folder paths  OutputFolder <- '/Users/denaclink/Desktop/JahooArray/ResNet50Unfrozen/FemaleDetections/'  OutputFolderSelections <- '/Users/denaclink/Desktop/JahooArray/ResNet50Unfrozen/FemaleSelectionTables/'  OutputFolderWav <- '/Users/denaclink/Desktop/JahooArray/ResNet50Unfrozen/FemaleWavs/'#
  # Import trained model  # Chose because had highest precision while maintaining recall   modelAlexNetGibbon <- luz_load("/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin/_imagescambodiaFP_4_modelResNet50.pt")#
  # path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustic Gibbon PAM 27_09_22' # Do not run this again because T/F positives split#
  # path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM 15_03_22'#
  path.to.files <- '/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM 15_12_22'  # Already run above this line ##
  #   # path.to.files <-'/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM_04_03_23'  # Doesn't have all 10 #/Users/denaclink/Library/CloudStorage/Box-Box/Cambodia 2022/Acoustics Gibbon PAM_08_07_23'#
  # Get a list of full file paths of sound files in a directory  SoundFilePathFull <- list.files(path.to.files,                                  recursive = T, pattern = '.wav', full.names = T)#
  # Get a list of file names of sound files in a directory  SoundFilePathShort <- list.files(path.to.files,                                   recursive = T, pattern = '.wav')#
  # Extract only the file names without the extension  SoundFilePathShort <- str_split_fixed(SoundFilePathShort,                                        pattern = '.wav', n = 2)[,1]#
  # Count the number of slashes in the first file path  nslash <- str_count(SoundFilePathShort[1], '/') + 1#
  # Split the file paths based on slashes and keep the last part (file name)  SoundFilePathShort <- str_split_fixed(SoundFilePathShort,                                        pattern = '/', n = nslash)[, nslash]#
  # Extract the times from the file names  times <- as.numeric(substr(str_split_fixed(SoundFilePathShort, pattern = '_', n = 3)[, 3], 1, 2))#
  # Select the indices of times that are between 6 and 10   times.index <- which(times == 5 | times == 6 | times == 7 | times == 7 )#
  # Filter the full file paths and file names based on the selected indices  SoundFilePathFull <- SoundFilePathFull[times.index]  SoundFilePathShort <- SoundFilePathShort[times.index]#
  # Set parameters for processing sound clips  clip.duration <- 12       # Duration of each sound clip  hop.size <- 6             # Hop size for splitting the sound clips  downsample.rate <- 'NA'   # Downsample rate for audio in Hz, otherwise NA  threshold <- 0.5         # Threshold for audio detection  sav.wav <- T              # Save the extracted sound clips as WAV files?#
  # Create output folders if they don't exist  dir.create(OutputFolder, recursive = TRUE, showWarnings = FALSE)  dir.create(OutputFolderSelections, recursive = TRUE, showWarnings = FALSE)  dir.create(OutputFolderWav, recursive = TRUE, showWarnings = FALSE)#
  #which(str_detect( SoundFilePathShort,'R1053_20220512_05000'))  for(x in (681:length(SoundFilePathFull)) ){ tryCatch({    RavenSelectionTableDFAlexNet <- data.frame()#
    start.time.detection <- Sys.time()    print(paste(x, 'out of', length(SoundFilePathFull)))    TempWav <- readWave(SoundFilePathFull[x])    WavDur <- duration(TempWav)#
    Seq.start <- list()    Seq.end <- list()#
    i <- 1    while (i + clip.duration < WavDur) {      # print(i)      Seq.start[[i]] = i      Seq.end[[i]] = i+clip.duration      i= i+hop.size    }#
    ClipStart <- unlist(Seq.start)    ClipEnd <- unlist(Seq.end)#
    TempClips <- cbind.data.frame(ClipStart,ClipEnd)#
    # Subset sound clips for classification -----------------------------------    print('saving sound clips')    set.seed(13)    length <- nrow(TempClips)#
    if(length > 100){    length.files <- seq(1,length,100)    } else {      length.files <- c(1,length)    }#
    for(q in 1: (length(length.files)-1) ){      unlink('/Users/denaclink/Desktop/datacopy/Temp/WavFiles', recursive = TRUE)      unlink('/Users/denaclink/Desktop/datacopy/Temp/Images/Images', recursive = TRUE)#
      dir.create('/Users/denaclink/Desktop/datacopy/Temp/WavFiles')      dir.create('/Users/denaclink/Desktop/datacopy/Temp/Images/Images')#
      RandomSub <-  seq(length.files[q],length.files[q+1],1)#
      if(q== (length(length.files)-1) ){        RandomSub <-  seq(length.files[q],length,1)      }#
      start.time <- TempClips$ClipStart[RandomSub]      end.time <- TempClips$ClipEnd[RandomSub]#
      short.sound.files <- lapply(1:length(start.time),                                  function(i)                                    extractWave(                                      TempWav,                                      from = start.time[i],                                      to = end.time[i],                                      xunit = c("time"),                                      plot = F,                                      output = "Wave"                                    ))#
      if(downsample.rate != 'NA'){      print('downsampling')      short.sound.files <- lapply(1:length(short.sound.files),                                  function(i)                                    downsample(                                      short.sound.files[[i]],                                      samp.rate=downsample.rate                                    ))      }#
      for(d in 1:length(short.sound.files)){        #print(d)        writeWave(short.sound.files[[d]],paste('/Users/denaclink/Desktop/datacopy/Temp/WavFiles','/',                                               SoundFilePathShort[x],'_',start.time[d], '.wav', sep=''),                  extensible = F)      }#
      # Save images to a temp folder      print(paste('Creating images',start.time[1],'start time clips'))#
      for(e in 1:length(short.sound.files)){        jpeg(paste('/Users/denaclink/Desktop/datacopy/Temp/Images/Images','/', SoundFilePathShort[x],'_',start.time[e],'.jpg',sep=''),res = 50)        short.wav <- short.sound.files[[e]]#
        seewave::spectro(short.wav,tlab='',flab='',axisX=F,axisY = F,scale=F,grid=F,flim=c(0.4,3),fastdisp=TRUE,noisereduction=1)#
        graphics.off()      }#
      # Predict using AlexNet ----------------------------------------------------      print('Classifying images using AlexNet')#
     test.input <- '/Users/denaclink/Desktop/datacopy/Temp/Images/'#
     # ResNet     test_ds <- image_folder_dataset(       file.path(test.input),       transform = . %>%         torchvision::transform_to_tensor() %>%         torchvision::transform_resize(size = c(224, 224)) %>%         torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),       target_transform = function(x) as.double(x) - 1)#
     # AlexNet and VGG19     # test_ds <- image_folder_dataset(     #    file.path(test.input ),     #    transform = . %>%     #      torchvision::transform_to_tensor() %>%     #      torchvision::transform_color_jitter() %>%     #      transform_resize(256) %>%     #      transform_center_crop(224) %>%     #      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )     #  #
      # Predict the test files      # Variable indicating the number of files      nfiles <- test_ds$.length()#
      # Load the test images      test_dl <- dataloader(test_ds, batch_size =nfiles)#
      # Predict using AlexNet      AlexNetPred <- predict(modelAlexNetGibbon, test_dl)      AlexNetProb <- torch_sigmoid(AlexNetPred)      AlexNetProb <- 1-as_array(torch_tensor(AlexNetProb,device = 'cpu'))      AlexNetClass <- ifelse((AlexNetProb) >= threshold, "Gibbon", "noise")#
      # Calculate the probability associated with each class      Probability <- AlexNetProb#
      image.files <- list.files(file.path(test.input),recursive = T,                                full.names = T)      nslash <- str_count(image.files,'/')+1      nslash <- nslash[1]      image.files.short <- str_split_fixed(image.files,pattern = '/',n=nslash)[,nslash]      image.files.short <- str_split_fixed(image.files.short,pattern = '.jpg',n=2)[,1]#
      print('Saving output')      Detections <-  which(Probability >= threshold )#
      Detections <-  split(Detections, cumsum(c(        1, diff(Detections)) != 1))#
      for(i in 1:length(Detections)){        TempList <- Detections[[i]]        if(length(TempList)==1){          Detections[[i]] <- TempList[1]        }        if(length(TempList)==2){          Detections[[i]] <- TempList[2]        }        if(length(TempList)> 2){          Detections[[i]] <- median(TempList)        }#
      }#
      DetectionIndices <- unname(unlist(Detections))#
      print('Saving output')      file.copy(image.files[DetectionIndices],                to= paste(OutputFolder,                          image.files.short[DetectionIndices],                          '_',                          round(Probability[DetectionIndices],2),                          '_AlexNet_.jpg', sep=''))#
      if(sav.wav ==T){        wav.file.paths <- list.files('/Users/denaclink/Desktop/datacopy/Temp/WavFiles',full.names = T)        file.copy(wav.file.paths[DetectionIndices],                  to= paste(OutputFolderWav,                            image.files.short[DetectionIndices],                            '_',                            round(Probability[DetectionIndices],2),                            '_AlexNet_.wav', sep=''))      }#
      Detections <- image.files.short[DetectionIndices]#
      if (length(Detections) > 0) {        Selection <- seq(1, length(Detections))        View <- rep('Spectrogram 1', length(Detections))        Channel <- rep(1, length(Detections))        MinFreq <- rep(100, length(Detections))        MaxFreq <- rep(2000, length(Detections))        start.time.new <- as.numeric(str_split_fixed(Detections,pattern = '_',n=4)[,4])        end.time.new <- start.time.new + clip.duration        Probability <- round(Probability[DetectionIndices],2)#
        RavenSelectionTableDFAlexNetTemp <-          cbind.data.frame(Selection,                           View,                           Channel,                           MinFreq,                           MaxFreq,start.time.new,end.time.new,Probability,                           Detections)#
        RavenSelectionTableDFAlexNetTemp <-          RavenSelectionTableDFAlexNetTemp[, c(            "Selection",            "View",            "Channel",            "start.time.new",            "end.time.new",            "MinFreq",            "MaxFreq",            'Probability',"Detections"          )]#
        colnames(RavenSelectionTableDFAlexNetTemp) <-          c(            "Selection",            "View",            "Channel",            "Begin Time (s)",            "End Time (s)",            "Low Freq (Hz)",            "High Freq (Hz)",            'Probability',            "Detections"          )#
        RavenSelectionTableDFAlexNet <- rbind.data.frame(RavenSelectionTableDFAlexNet,                                                       RavenSelectionTableDFAlexNetTemp)#
        if(nrow(RavenSelectionTableDFAlexNet) > 0){          csv.file.name <-            paste(OutputFolderSelections,                  SoundFilePathShort[x],                  'GibbonAlexNetAllFilesMalaysia.txt',                  sep = '')#
          write.table(            x = RavenSelectionTableDFAlexNet,            sep = "\t",            file = csv.file.name,            row.names = FALSE,            quote = FALSE          )          print(paste(            "Saving Selection Table"          ))        }#
      }    }#
    if(nrow(RavenSelectionTableDFAlexNet) == 0){      csv.file.name <-        paste(OutputFolderSelections,              SoundFilePathShort[x],              'GibbonAlexNetAllFilesMalaysia.txt',              sep = '')#
     ColNames <-  c(       "Selection",       "View",       "Channel",       "Begin Time (s)",       "End Time (s)",       "Low Freq (Hz)",       "High Freq (Hz)",       'Probability',       "Detections"     )#
      TempNARow <- t(as.data.frame(rep(NA,length(ColNames))))#
      colnames(TempNARow) <- ColNames#
      write.table(        x = TempNARow,        sep = "\t",        file = csv.file.name,        row.names = FALSE,        quote = FALSE      )      print(paste(        "Saving Selection Table"      ))    }    rm(TempWav)    rm(short.sound.files)    rm( test_ds )    rm(short.wav)    end.time.detection <- Sys.time()    print(end.time.detection-start.time.detection)    gc()  }, error = function(e) { cat("ERROR :", conditionMessage(e), "\n") })  }
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)#
TrainedModels <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_frozenbin',           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_frozenbin',                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in c(73:144)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1 - as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images #input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodia','/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodiaFP', '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysia','/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')epoch.iterations <
- c(20,50)#
for(a in 1:length(input.data)){  trainingfolder <- str_split_fixed(input.data[a],pattern = '/',n=2)[,2]#
  for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(TRUE) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelAlex
Net.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(TRUE)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(TRUE)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Pro
bdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch,'_',a, '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("
modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load(
"modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_
load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <-
seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch, '_', a, '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}#
}
trainingfolder
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images #input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodia','/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodiaFP', '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysia','/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')epoch.iterations
<- c(20,50)#
for(a in 1:length(input.data)){  trainingfolder <-'imagesmalaysiaHQ'# str_split_fixed(input.data[a],pattern = '/',n=2)[,2]#
  for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(TRUE) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelAlex
Net.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(TRUE)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(TRUE)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Pro
bdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch,'_',a, '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("
modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load(
"modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_
load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <-
seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch, '_', a, '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}#
}
# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/D
esktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch,'_',a, '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)rm(modelVGG19Gibbon)
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("
modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load(
"modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_
load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <-
seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch, '_', a, '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images #input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodia','/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodiaFP', '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysia','/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')epoch.iterations
<- c(50)#
for(a in 1:length(input.data)){  trainingfolder <-'imagesmalaysiaHQ'# str_split_fixed(input.data[a],pattern = '/',n=2)[,2]#
  for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(TRUE) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelAlex
Net.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(TRUE)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(TRUE)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Pro
bdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch,'_',a, '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)rm(modelVGG19Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data[a],'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data[a], "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("
modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load(
"modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(TRUE) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, n.classes)    )  },  forward = function(x) {    self$model(x)[,1]  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_noearly/"),        luz_callback_csv_logger(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_
load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/',trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(input.data[a],'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <-
seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/', trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly/performance_tables/', trainingfolder, '_', n.epoch, '_', a, '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}#
}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)#
TrainedModels <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly',           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_noearly',                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 7:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1 - as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_noearly/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)#
TrainedModels <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single',           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single',                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 1:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1 - as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means th
e features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(5)#
trainingfolder <- 'imagesmalaysiaHQ'#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {
unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_single/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV
.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_single/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCS
V.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_single/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss
<- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Ima
ges/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)#
TrainedModels <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single',           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single',                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 25:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1 - as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}#performance_scores <- read.csv('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/performance_scores_dffrozen.csv')performance_scores <- read.csv('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/performance_scores_dfunfrozen_randomsamples.csv')unique_training_data <- unique(performance_scores$TrainingData)performance_scores <- subset(performance_scores, Threshold == .5   )best_f1_results <- data.frame()# Loop through each 'TrainingData' type and find the row with the maximum F1 scorefor (td in unique_training_data) {  subset_data <- subset(performance_scores, TrainingData == td   )  max_f1_row <- subset_data[which.
max(subset_data$F1), ]  best_f1_results <- rbind.data.frame(best_f1_results, max_f1_row)}# Print the best F1 scores for each 'TrainingData'print(best_f1_results)unique_training_data <- unique(performance_scores$TrainingData)unique_model_type <- unique(performance_scores$ModelType)best_f1_results_comb <- data.frame()# Loop through each 'TrainingData' type and find the row with the maximum F1 scorefor (a in 1:length(unique_model_type)) {  TempDF <-  subset(performance_scores, ModelType == unique_model_type[a])#
  for (b in 1:length(unique_training_data)) {    subset_data <- subset(TempDF, TrainingData == unique_training_data[b])    max_f1_row <- subset_data[which.max(subset_data$F1), ]    best_f1_results_comb <- rbind.data.frame(best_f1_results_comb, max_f1_row)  }#
}# Print the best F1 scores for each 'TrainingData'print(best_f1_results_comb)ggpubr::ggboxplot(data=performance_scores,color='ModelType',y='F1',x='Threshold',facet.by = 'TrainingData')#
performance_scores_sub <- subset(performance_scores, NEpochs=='20')performance_scores_sub$Recall <- round(as.numeric(performance_scores_sub$Recall),1)ggpubr::ggline(data=performance_scores_sub,color='ModelType',y='F1',x='Threshold',facet.by = 'TrainingData')#ggpubr::ggline(data=performance_scores_sub,color='ModelType',y='Precision',x='Recall',facet.by = 'TrainingData')
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau')test.data.short <- 'imagesmalaysiamaliau'# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single/'# Whether to unfreeze the lay
ersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(20)#
trainingfolder <- 'imagesmalaysiaHQ'#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {
unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_single/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCS
V.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_single/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempC
SV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        #luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_single/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss
<- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Im
ages/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_single/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/imagesmalaysiamaliau')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images_balanced'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per y
our model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19
Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.ResNet
18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.ResNe
t50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <- Temp
CSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images',
recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/finaltest')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_D
ata_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gibbon <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19
Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gibbon <- fitted# Save model outputluz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.ResNet
18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gibbon <- fitted# Save model outputluz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.ResNe
t50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gibbon <- fitted# Save model outputluz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <- Temp
CSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images',
recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gibbon)rm(modelResNet50Gibbon)rm(modelResNet152Gibbon)}
# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1:length(imageFiles), 1)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)
# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'
# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1:length(imageFiles), 1)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', re
cursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGibbon)rm(modelVGG16Gibbon)
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/test')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training
_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1:length(imageFiles), 1)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Pro
bdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gunshots", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gunshots", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gunshots", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gunshots", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gunshots", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gunshots", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.Re
sNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.R
esNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <-
TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Image
s', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gunshots", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gunshots", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gunshots", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gunshots", "Noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gunshots", "Noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gunshots", "Noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')
# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 1)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq))) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gunshots", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gunshots", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gunshots", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gunshots", "Noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gunshots", "Noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gunshots", "Noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)
# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 1)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq))) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)
modelAlexNetGunshot
modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Dat
a_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 1)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Pro
bdf <- data.frame()for (i in 1:(length(ImageFilesSeq))) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.Re
sNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.R
esNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <-
TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 100)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq))) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images'
, recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
modelAlexNetGunshot
# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 5)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq))) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recu
rsive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)
for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet -------------------------------
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Dat
a_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 5)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Pro
bdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.Re
sNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.R
esNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <-
TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 5)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images'
, recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Dat
a_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 3:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 5)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Pro
bdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.
ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV
.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <
- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 5)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Image
s', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
ResNet152Perf
outputTableResNet152
nrow(ResNet152Probdf)
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data
_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 3:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 5)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Pro
bdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.Re
sNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.R
esNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <-
TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 5)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images'
, recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data
_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1,length(imageFiles), 5)AlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Pro
bdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images', recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = TempLong, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = TempLong, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = TempLong, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(AlexNetProb > 0.9)],  #           to= paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/',TempShort[which(AlexNetProb > 0.9)], sep=''))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)}# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.Re
sNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV.R
esNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <-
TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()# Iterate over image filesImageFilesSeq <- seq(1, length(imageFiles), 5)ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()for (i in 1:(length(ImageFilesSeq)-1)) {  unlink('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images'
, recursive = TRUE)  dir.create('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images')#
  print(paste(output.data.path, 'processing', i, 'out of', length(ImageFilesSeq)))#
  batchSize <- length(ImageFilesSeq)#
  TempLong <- imageFiles[ImageFilesSeq[i]:ImageFilesSeq[i+1]]  TempShort <- imageFileShort[ImageFilesSeq[i]:ImageFilesSeq[i+1]]#
  # Load and preprocess the image  file.copy(TempLong,            to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort, sep = ''))#
  test.input <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/'#
  test_ds <- image_folder_dataset(    file.path(test.input, "Images/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))#
  # Save false positives to train next iteration  # file.copy(TempLong[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)}#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = batchSize)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")
# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = nfiles)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = TempLong, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = TempLong, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = TempLong, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder[ImageFilesSeq[i]:ImageFilesSeq[i+1]]))
outputTableResNet152
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'images'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Dat
a_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- TempCSV.
ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <- TempCSV
.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152.loss <
- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_balanced'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name
Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- T
empCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <-
TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152
.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_balanced'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per
your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- T
empCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <-
TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152
.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_balanced'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per
your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- T
empCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <-
TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152
.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_balanced'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per
your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.loss <- T
empCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.loss <-
TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_balanced/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))ResNet152
.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_balanced/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_trainaddedclean/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_trainaddedclean/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_trainaddedclean/'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_trainaddedclean'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50,
ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Unfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.lo
ss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50.l
oss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are frozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_frozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))Re
sNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_frozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_trainaddedclean/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_trainaddedclean/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin_trainaddedclean/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are unfrozen; TRUE ununfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_trainaddedclean'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResN
et50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are unfrozen; TRUE ununfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$unfrozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are unfrozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.
loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are unfrozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50
.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are unfrozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))
ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$unfrozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_trainaddedclean/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_trainaddedclean/')# Location to save the outoutput.data.path <- '/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin_trainaddedclean/'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5)trainingfolder <- 'images_trainaddedclean'# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50
, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){n.epoch <- epoch.iterations[b]# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_color_jitter() %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(224, 224)) %>%    torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)n.classes <- length(train_ds$class_to_idx)classes <- train_ds$classes# Train AlexNet -----------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_alexnet(pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(9216, 512),      nn_relu(),      nn_linear(512, 256),      nn_relu(),      nn_linear(256, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  }#
)#
fitted <- net %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelAlexNetGunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
# Train VGG16 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg16 (pretrained = TRUE)    for (par in self$parameters) {     par$requires_grad_(unfreeze.param)    }    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG16Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,         epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG16/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss# Train VGG19 -------------------------------------------------------------net <- torch::nn_module(  initialize = function() {    self$model <- model_vgg19 (pretrained = TRUE)#
    for (par in self$parameters) {      par$requires_grad_(unfreeze.param)    }#
    self$model$classifier <- nn_sequential(      nn_dropout(0.5),      nn_linear(25088, 4096),      nn_relu(),      nn_dropout(0.5),      nn_linear(4096, 4096),      nn_relu(),      nn_linear(4096, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })fitted <- net  %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
modelVGG19Gunshot <- fitted %>%  fit(train_dl,epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"        ),        #luz_callback_model_checkpoint(path = "cpt_VGG19/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))      ),      verbose = TRUE)# Save model outputluz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss# Calculate performance metrics -------------------------------------------dir.create(paste(output.data.path,'performance_tables',sep=''))# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableAlexNet <- data.frame()outputTableVGG16 <- data.frame()outputTableVGG19 <- data.frame()# Iterate over image filesAlexNetProbdf <- data.frame()VGG16Probdf <- data.frame()VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # AlexNet  AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "gunshot", "noise")#
  AlexNetPerf <- caret::confusionMatrix(    as.factor(AlexNetPredictedClass),    as.factor(outputTableAlexNet$ActualClass),    mode = 'everything'  )$byClass#
  TempRowAlexNet <- cbind.data.frame(    t(AlexNetPerf[5:7]),    AlexNet.loss,    trainingfolder,    n.epoch,    'AlexNet'  )#
  colnames(TempRowAlexNet) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowAlexNet$Threshold <- as.character(threshold)#
  # VGG16  VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "gunshot", "noise")#
  VGG16Perf <- caret::confusionMatrix(    as.factor(VGG16PredictedClass),    as.factor(outputTableVGG16$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG16 <- cbind.data.frame(    t(VGG16Perf[5:7]),    VGG16.loss,    trainingfolder,    n.epoch,    'VGG16'  )#
  colnames(TempRowVGG16) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG16$Threshold <- as.character(threshold)#
  # VGG19  VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "gunshot", "noise")#
  VGG19Perf <- caret::confusionMatrix(    as.factor(VGG19PredictedClass),    as.factor(outputTableVGG19$ActualClass),    mode = 'everything'  )$byClass#
  TempRowVGG19 <- cbind.data.frame(    t(VGG19Perf[5:7]),    VGG19.loss,    trainingfolder,    n.epoch,    'VGG19'  )#
  colnames(TempRowVGG19) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowVGG19$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelAlexNetGunshot)rm(modelVGG16Gunshot)rm(modelVGG19Gunshot)#
# Start ResNet ------------------------------------------------------------TransferLearningCNNDF <- data.frame()# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data,'train' ),  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_color_jitter() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )valid_ds <- image_folder_dataset(  file.path(input.data, "valid"),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(256) %>%    transform_center_crop(224) %>%    transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
# Train ResNet18 -----------------------------------------------------------#
convnet <- nn_module(  initialize = function() {    self$model <- model_resnet18 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are unfrozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet18Gunshot <- fitted# Save model outputluz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))#modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))ResNet18.
loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
# Train ResNet50 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet50 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are unfrozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet50Gunshot <- fitted# Save model outputluz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))#modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))ResNet50
.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss# Train ResNet152 -------------------------------------------------------------convnet <- nn_module(  initialize = function() {    self$model <- model_resnet152 (pretrained = TRUE)    for (par in self$parameters) {      par$requires_grad_(unfreeze.param) # False means the features are unfrozen    }    self$model$fc <- nn_sequential(      nn_linear(self$model$fc$in_features, 1024),      nn_relu(),      nn_linear(1024, 1024),      nn_relu(),      nn_linear(1024, 1)    )  },  forward = function(x) {    output <- self$model(x)    torch_squeeze(output, dim=2)  })model <- convnet %>%  setup(    loss = nn_bce_with_logits_loss(),    optimizer = optim_adam,    metrics = list(      luz_metric_binary_accuracy_with_logits()    )  )#
# rates_and_losses <- model %>% lr_finder(train_dl)# rates_and_losses %>% plot()fitted <- model %>%  fit(train_dl, epochs=n.epoch, valid_data = valid_dl,      callbacks = list(        # luz_callback_early_stopping(patience = 2),        luz_callback_lr_scheduler(          lr_one_cycle,          max_lr = 0.01,          epochs=n.epoch,          steps_per_epoch = length(train_dl),          call_on = "on_batch_end"),        #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),        luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))      ),      verbose = TRUE)# Save model outputmodelResNet152Gunshot <- fitted# Save model outputluz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))#modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))
ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss# Calculate performance metrics -------------------------------------------# Get the list of image filesimageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)# Get the list of image filesimageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#imageFileShort <- paste( Folder,imageFileShort,sep='')# Prepare output tablesoutputTableResNet18 <- data.frame()outputTableResNet50 <- data.frame()outputTableResNet152 <- data.frame()ResNet18Probdf <- data.frame()ResNet50Probdf <- data.frame()ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
# Initialize data framesCombinedTempRow <- data.frame()TransferLearningCNNDF <- data.frame()# Threshold values to considerthresholds <- c(0.95, 0.85, 0.50)for (threshold in thresholds) {  # ResNet18  ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "gunshot", "noise")#
  ResNet18Perf <- caret::confusionMatrix(    as.factor(ResNet18PredictedClass),    as.factor(outputTableResNet18$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet18 <- cbind.data.frame(    t(ResNet18Perf[5:7]),    ResNet18.loss,    trainingfolder,    n.epoch,    'ResNet18'  )#
  colnames(TempRowResNet18) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet18$Threshold <- as.character(threshold)#
  # ResNet50  ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "gunshot", "noise")#
  ResNet50Perf <- caret::confusionMatrix(    as.factor(ResNet50PredictedClass),    as.factor(outputTableResNet50$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet50 <- cbind.data.frame(    t(ResNet50Perf[5:7]),    ResNet50.loss,    trainingfolder,    n.epoch,    'ResNet50'  )#
  colnames(TempRowResNet50) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet50$Threshold <- as.character(threshold)#
  # ResNet152  ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "gunshot", "noise")#
  ResNet152Perf <- caret::confusionMatrix(    as.factor(ResNet152PredictedClass),    as.factor(outputTableResNet152$ActualClass),    mode = 'everything'  )$byClass#
  TempRowResNet152 <- cbind.data.frame(    t(ResNet152Perf[5:7]),    ResNet152.loss,    trainingfolder,    n.epoch,    'ResNet152'  )#
  colnames(TempRowResNet152) <- c(    "Precision",    "Recall",    "F1",    "Validation loss",    "Training Data",    "N epochs",    "CNN Architecture"  )#
  TempRowResNet152$Threshold <- as.character(threshold)#
  CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)  CombinedTempRowThreshold$Threshold <- as.character(threshold)#
  # Append to the overall result data frame  CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)}# Append to the main data frameTransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)TransferLearningCNNDF$Frozen <- unfreeze.param# Write the result to a CSV filefilename <- paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')write.csv(TransferLearningCNNDF, filename, row.names = FALSE)rm(modelResNet18Gunshot)rm(modelResNet50Gunshot)rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau/')# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)trainingfolder <- 'imagesmalaysiaHQ'# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(output.data.path)
# Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbon", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbon", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbon", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- c(0.95, 0.85, 0.50)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbon", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbon", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbon", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','finaltest',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "finaltest/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbon", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbon", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbon", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- c(0.95, 0.85, 0.50)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbon", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbon", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbon", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau/')# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Training data folder shorttrainingfolder <- 'imagesmalaysiaHQ'# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't exist
dir.create(output.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbon", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbon", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbon", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.5,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbon", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbon", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbon", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbon", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbon", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbon", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.5,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbon", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbon", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbon", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
AlexNetPredictedClass
outputTableAlexNet$ActualClass
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau/')# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Training data folder shorttrainingfolder <- 'imagesmalaysiaHQ'# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't exist
dir.create(output.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch,'_', '_TransferLearningCNNDFAlexNetVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/output_unfrozenbin_trainaddedclean/performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau/')# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Training data folder shorttrainingfolder <- 'imagesmalaysiaHQ'# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't exist
dir.create(output.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau/')# Training data folder shorttrainingfolder <- 'imagesmalaysiaHQ'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't exist
dir.create(output.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- 1 - as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- 1 - as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- 1 - as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) > threshold, "Gibbons", "Noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) > threshold, "Gibbons", "Noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) > threshold, "Gibbons", "Noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- 1 - as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- 1 - as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) > 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- 1 - as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) > 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) > threshold, "Gibbons", "Noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) > threshold, "Gibbons", "Noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) > threshold, "Gibbons", "Noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiaHQ/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesmalaysiamaliau/')# Training data folder shorttrainingfolder <- 'imagesmalaysiaHQ'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't exist
dir.create(output.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "Gibbons", "Noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "Gibbons", "Noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "Gibbons", "Noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(         luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "Gibbons", "Noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "Gibbons", "Noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "Gibbons", "Noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)#
TrainedModels <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/_output_unfrozen_TRUE_imagesmalaysiaHQ_',           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/_output_unfrozen_TRUE_imagesmalaysiaHQ_',                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 25:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)#
TrainedModels <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/_output_unfrozen_TRUE_imagesmalaysiaHQ_',           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/_output_unfrozen_TRUE_imagesmalaysiaHQ_',                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 1:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/finaltest/')# Training data folder shorttrainingfolder <- 'images_balanced'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(output.data.path)
# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "gunshot", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "gunshot", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "gunshot", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGunshot)  rm(modelVGG16Gunshot)  rm(modelVGG19Gunshot)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gunshot <- fitted#
  # Save model output  luz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gunshot <- fitted#
  # Save model output  luz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gunshot <- fitted#
  # Save model output  luz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "gunshot", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "gunshot", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "gunshot", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gunshot)  rm(modelResNet50Gunshot)  rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/finaltest')# Training data folder shorttrainingfolder <- 'images_balanced'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(outp
ut.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "gunshot", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "gunshot", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "gunshot", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGunshot)  rm(modelVGG16Gunshot)  rm(modelVGG19Gunshot)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gunshot <- fitted#
  # Save model output  luz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gunshot <- fitted#
  # Save model output  luz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gunshot <- fitted#
  # Save model output  luz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "gunshot", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "gunshot", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "gunshot", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gunshot)  rm(modelResNet50Gunshot)  rm(modelResNet152Gunshot)}
test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/finaltest')# Training data folder shorttrainingfolder <- 'images_balanced'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(outp
ut.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "gunshot", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "gunshot", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "gunshot", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGunshot)  rm(modelVGG16Gunshot)  rm(modelVGG19Gunshot)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gunshot <- fitted#
  # Save model output  luz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gunshot <- fitted#
  # Save model output  luz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gunshot <- fitted#
  # Save model output  luz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "gunshot", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "gunshot", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "gunshot", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gunshot)  rm(modelResNet50Gunshot)  rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/finaltest')# Training data folder shorttrainingfolder <- 'images_balanced'# Whether to unfreeze the layersunfreeze.param <- FALSE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(out
put.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "gunshot", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "gunshot", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "gunshot", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGunshot)  rm(modelVGG16Gunshot)  rm(modelVGG19Gunshot)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gunshot <- fitted#
  # Save model output  luz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gunshot <- fitted#
  # Save model output  luz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gunshot <- fitted#
  # Save model output  luz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "gunshot", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "gunshot", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "gunshot", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gunshot)  rm(modelResNet50Gunshot)  rm(modelResNet152Gunshot)}
ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)
modelResNet18Gunshot <- luz_load(paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))
ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)
ResNet18Pred
torch_sigmoid(ResNet18Pred)
1-torch_sigmoid(ResNet18Pred)
ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images_balanced/finaltest')# Training data folder shorttrainingfolder <- 'images_balanced'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(outp
ut.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "gunshot", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "gunshot", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "gunshot", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGunshot)  rm(modelVGG16Gunshot)  rm(modelVGG19Gunshot)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gunshot <- fitted#
  # Save model output  luz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")  modelResNet18Gunshot <- luz_load(paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gunshot <- fitted#
  # Save model output  luz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gunshot <- fitted#
  # Save model output  luz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "gunshot", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "gunshot", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "gunshot", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gunshot)  rm(modelResNet50Gunshot)  rm(modelResNet152Gunshot)}
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/images/combinedtest')# Training data folder shorttrainingfolder <- 'images'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.create(output.data.path)# Allow e
arly stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {    self$model(x)[,1]  }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGunshot, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGunshot <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      self$model(x)[,1]    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      self$model(x)[,1]    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gunshot <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGunshot, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "gunshot", "noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gunshot, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "gunshot", "noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gunshot, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "gunshot", "noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "gunshot", "noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "gunshot", "noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGunshot)  rm(modelVGG16Gunshot)  rm(modelVGG19Gunshot)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      self$model(x)[,1]    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gunshot <- fitted#
  # Save model output  luz_save(modelResNet18Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gunshot <- luz_load("modelResNet18Gunshot1epochs.pt")  modelResNet18Gunshot <- luz_load(paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      self$model(x)[,1]    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gunshot <- fitted#
  # Save model output  luz_save(modelResNet50Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gunshot <- luz_load("modelResNet50Gunshot1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      self$model(x)[,1]    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gunshot <- fitted#
  # Save model output  luz_save(modelResNet152Gunshot, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gunshot <- luz_load("modelResNet152Gunshot1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(test.data, recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(test.data, recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gunshot, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gunshot, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "gunshot", "noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gunshot, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "gunshot", "noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Vietnam-Gunshots/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "gunshot", "noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "gunshot", "noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "gunshot", "noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gunshot)  rm(modelResNet50Gunshot)  rm(modelResNet152Gunshot)}
TransferLearningCNNDF
filename
# Combine training and testing into one script# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)library(torchdatasets)library(stringr)library(ROCR)library(dplyr)library(tibble)library(readr)#
# Datasets ----------------------------------------------------------------# Note: Need to create temp folders in project  to save imagesdevice <- if(cuda_is_available()) "cuda" else "cpu"to_device <- function(x, device) {  x$to(device = device)}# Location of spectrogram images for training input.data <-  c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagescambodiaFP/')# Location of spectrogram images for testingtest.data <- c('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/imagesVietnam/')# Training data folder shorttrainingfolder <- 'imagescambodiaFP'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Location to save the outoutput.data.path <-paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/','output','unfrozen',unfreeze.param,trainingfolder,'/', sep='_')# Create if doesn't existdir.cr
eate(output.data.path)# Allow early stopping?early.stop <- 'yes' # NOTE: Must comment out if don't want early stopping                          # Create metadatametadata <- tibble(  Model_Name = "AlexNet, VGG16, VGG19, ResNet18, ResNet50, ResNet152", # Modify as per your model's name  Training_Data_Path = input.data,  Test_Data_Path = test.data,  Output_Path = output.data.path,  Device_Used = device,  EarlyStop=early.stop,  Layers_Ununfrozen = unfreeze.param,  Epochs = epoch.iterations)# Save metadata to CSVwrite_csv(metadata, paste0(output.data.path, "model_metadata.csv"))#
for(b in 1:length(epoch.iterations)){#
  n.epoch <- epoch.iterations[b]#
  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_color_jitter() %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1)#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  n.classes <- length(train_ds$class_to_idx)  classes <- train_ds$classes#
  # Train AlexNet -----------------------------------------------------------#
  net <- torch::nn_module(#
    initialize = function() {      self$model <- model_alexnet(pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # FALSE means the features are frozen; TRUE unfrozen      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(9216, 512),        nn_relu(),        nn_linear(512, 256),        nn_relu(),        nn_linear(256, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }#
  )#
  fitted <- net %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelAlexNetGibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "cpt_AlexNet/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelAlexNetGibbon, paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))  #modelAlexNetGibbon <- luz_load( paste( output.data.path,trainingfolder,n.epoch, "modelAlexNet.pt",sep='_'))#
  TempCSV.AlexNet <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_AlexNet.csv",sep='_'))#
  AlexNet.loss <- TempCSV.AlexNet[nrow(TempCSV.AlexNet),]$loss#
  # Train VGG16 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg16 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG16Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG16/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG16Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG16.pt",sep='_'))#
  TempCSV.VGG16 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG16.csv",sep='_'))#
  VGG16.loss <- TempCSV.VGG16[nrow(TempCSV.VGG16),]$loss#
  # Train VGG19 -------------------------------------------------------------#
  net <- torch::nn_module(    initialize = function() {      self$model <- model_vgg19 (pretrained = TRUE)#
      for (par in self$parameters) {        par$requires_grad_(unfreeze.param)      }#
      self$model$classifier <- nn_sequential(        nn_dropout(0.5),        nn_linear(25088, 4096),        nn_relu(),        nn_dropout(0.5),        nn_linear(4096, 4096),        nn_relu(),        nn_linear(4096, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  fitted <- net  %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  modelVGG19Gibbon <- fitted %>%    fit(train_dl,epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"          ),          #luz_callback_model_checkpoint(path = "cpt_VGG19/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  luz_save(modelVGG19Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelVGG19.pt",sep='_'))#
  TempCSV.VGG19 <- read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_VGG19.csv",sep='_'))#
  VGG19.loss <- TempCSV.VGG19[nrow(TempCSV.VGG19),]$loss#
  # Calculate performance metrics -------------------------------------------  dir.create(paste(output.data.path,'performance_tables',sep=''))#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]  #imageFileShort <- paste( Folder,imageFileShort,sep='')#
  # Prepare output tables  outputTableAlexNet <- data.frame()  outputTableVGG16 <- data.frame()  outputTableVGG19 <- data.frame()#
  # Iterate over image files#
  AlexNetProbdf <- data.frame()  VGG16Probdf <- data.frame()  VGG19Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_resize(size = c(224, 224)) %>%      torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using AlexNet  AlexNetPred <- predict(modelAlexNetGibbon, test_dl)  AlexNetProb <- torch_sigmoid(AlexNetPred)  AlexNetProb <- as_array(torch_tensor(AlexNetProb, device = 'cpu'))  AlexNetClass <- ifelse((AlexNetProb) < 0.5, "Gibbons", "Noise")#
  # Predict using VGG16  VGG16Pred <- predict(modelVGG16Gibbon, test_dl)  VGG16Prob <- torch_sigmoid(VGG16Pred)  VGG16Prob <- as_array(torch_tensor(VGG16Prob, device = 'cpu'))  VGG16Class <- ifelse((VGG16Prob) < 0.5, "Gibbons", "Noise")#
  # Predict using VGG19  VGG19Pred <- predict(modelVGG19Gibbon, test_dl)  VGG19Prob <- torch_sigmoid(VGG19Pred)  VGG19Prob <- as_array(torch_tensor(VGG19Prob, device = 'cpu'))  VGG19Class <- ifelse((VGG19Prob) < 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableAlexNet <- rbind(outputTableAlexNet, data.frame(Label = Folder, Probability = AlexNetProb, PredictedClass = AlexNetClass, ActualClass = Folder))  outputTableVGG16 <- rbind(outputTableVGG16, data.frame(Label = Folder, Probability = VGG16Prob, PredictedClass = VGG16Class, ActualClass = Folder))  outputTableVGG19 <- rbind(outputTableVGG19, data.frame(Label = Folder, Probability = VGG19Prob, PredictedClass = VGG19Class, ActualClass = Folder))#
  # Save the output tables as CSV files  write.csv(outputTableAlexNet, paste(output.data.path, trainingfolder, n.epoch, "output_AlexNet.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG16, paste(output.data.path, trainingfolder, n.epoch, "output_VGG16.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableVGG19, paste(output.data.path, trainingfolder, n.epoch, "output_VGG19.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # AlexNet    AlexNetPredictedClass <- ifelse((outputTableAlexNet$Probability) < threshold, "Gibbons", "Noise")#
    AlexNetPerf <- caret::confusionMatrix(      as.factor(AlexNetPredictedClass),      as.factor(outputTableAlexNet$ActualClass),      mode = 'everything'    )$byClass#
    TempRowAlexNet <- cbind.data.frame(      t(AlexNetPerf[5:7]),      AlexNet.loss,      trainingfolder,      n.epoch,      'AlexNet'    )#
    colnames(TempRowAlexNet) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowAlexNet$Threshold <- as.character(threshold)#
    # VGG16    VGG16PredictedClass <- ifelse((outputTableVGG16$Probability) < threshold, "Gibbons", "Noise")#
    VGG16Perf <- caret::confusionMatrix(      as.factor(VGG16PredictedClass),      as.factor(outputTableVGG16$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG16 <- cbind.data.frame(      t(VGG16Perf[5:7]),      VGG16.loss,      trainingfolder,      n.epoch,      'VGG16'    )#
    colnames(TempRowVGG16) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG16$Threshold <- as.character(threshold)#
    # VGG19    VGG19PredictedClass <- ifelse((outputTableVGG19$Probability) < threshold, "Gibbons", "Noise")#
    VGG19Perf <- caret::confusionMatrix(      as.factor(VGG19PredictedClass),      as.factor(outputTableVGG19$ActualClass),      mode = 'everything'    )$byClass#
    TempRowVGG19 <- cbind.data.frame(      t(VGG19Perf[5:7]),      VGG19.loss,      trainingfolder,      n.epoch,      'VGG19'    )#
    colnames(TempRowVGG19) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowVGG19$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowAlexNet, TempRowVGG16, TempRowVGG19)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFAlexNETVGG16.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelAlexNetGibbon)  rm(modelVGG16Gibbon)  rm(modelVGG19Gibbon)#
  # Start ResNet ------------------------------------------------------------#
  TransferLearningCNNDF <- data.frame()  # Combined uses both  train_ds <- image_folder_dataset(    file.path(input.data,'train' ),    transform = . %>%      torchvision::transform_to_tensor() %>%      torchvision::transform_color_jitter() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  valid_ds <- image_folder_dataset(    file.path(input.data, "valid"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
  train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE, drop_last = TRUE)  valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE, drop_last = TRUE)#
  # Train ResNet18 -----------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet18 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch,"logs_ResNet18.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet18Gibbon <- fitted#
  # Save model output  luz_save(modelResNet18Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet18.pt",sep='_'))  #modelResNet18Gibbon <- luz_load("modelResNet18Gibbon1epochs.pt")#
  TempCSV.ResNet18 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet18.csv",sep='_'))#
  ResNet18.loss <- TempCSV.ResNet18[nrow(TempCSV.ResNet18),]$loss#
  # Train ResNet50 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet50 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet50Gibbon <- fitted#
  # Save model output  luz_save(modelResNet50Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet50.pt",sep='_'))  #modelResNet50Gibbon <- luz_load("modelResNet50Gibbon1epochs.pt")#
  TempCSV.ResNet50 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet50.csv",sep='_'))#
  ResNet50.loss <- TempCSV.ResNet50[nrow(TempCSV.ResNet50),]$loss  # Train ResNet152 -------------------------------------------------------------#
  convnet <- nn_module(    initialize = function() {      self$model <- model_resnet152 (pretrained = TRUE)      for (par in self$parameters) {        par$requires_grad_(unfreeze.param) # False means the features are unfrozen      }      self$model$fc <- nn_sequential(        nn_linear(self$model$fc$in_features, 1024),        nn_relu(),        nn_linear(1024, 1024),        nn_relu(),        nn_linear(1024, 1)      )    },    forward = function(x) {      output <- self$model(x)      torch_squeeze(output, dim=2)    }  )#
  model <- convnet %>%    setup(      loss = nn_bce_with_logits_loss(),      optimizer = optim_adam,      metrics = list(        luz_metric_binary_accuracy_with_logits()      )    )#
  # rates_and_losses <- model %>% lr_finder(train_dl)  # rates_and_losses %>% plot()#
  fitted <- model %>%    fit(train_dl, epochs=n.epoch, valid_data = valid_dl,        callbacks = list(          luz_callback_early_stopping(patience = 2),          luz_callback_lr_scheduler(            lr_one_cycle,            max_lr = 0.01,            epochs=n.epoch,            steps_per_epoch = length(train_dl),            call_on = "on_batch_end"),          #luz_callback_model_checkpoint(path = "output_unfrozenbin_trainaddedclean/"),          luz_callback_csv_logger(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))        ),        verbose = TRUE)#
  # Save model output  modelResNet152Gibbon <- fitted#
  # Save model output  luz_save(modelResNet152Gibbon, paste( output.data.path,trainingfolder,n.epoch, "modelResNet152.pt",sep='_'))  #modelResNet152Gibbon <- luz_load("modelResNet152Gibbon1epochs.pt")#
  TempCSV.ResNet152 <-  read.csv(paste( output.data.path,trainingfolder,n.epoch, "logs_ResNet152.csv",sep='_'))#
  ResNet152.loss <- TempCSV.ResNet152[nrow(TempCSV.ResNet152),]$loss#
  # Calculate performance metrics -------------------------------------------#
  # Get the list of image files  imageFiles <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = TRUE)#
  # Get the list of image files  imageFileShort <- list.files(paste(test.data,'/','test',sep=''), recursive = TRUE, full.names = FALSE)#
  Folder <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,1]#
  imageFileShort <- str_split_fixed( imageFileShort,pattern = '/',n=2)[,2]#
  # Prepare output tables  outputTableResNet18 <- data.frame()  outputTableResNet50 <- data.frame()  outputTableResNet152 <- data.frame()#
  ResNet18Probdf <- data.frame()  ResNet50Probdf <- data.frame()  ResNet152Probdf <- data.frame()#
  test_ds <- image_folder_dataset(    file.path(test.data, "test/"),    transform = . %>%      torchvision::transform_to_tensor() %>%      transform_resize(256) %>%      transform_center_crop(224) %>%      transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),    target_transform = function(x) as.double(x) - 1  )#
  # Predict the test files  # Variable indicating the number of files  #nfiles <- test_ds$.length()#
  # Load the test images  test_dl <- dataloader(test_ds, batch_size = 32, shuffle = F)#
  # Predict using ResNet18  ResNet18Pred <- predict(modelResNet18Gibbon, test_dl)  ResNet18Prob <- torch_sigmoid(ResNet18Pred)  ResNet18Prob <- as_array(torch_tensor(ResNet18Prob, device = 'cpu'))  ResNet18Class <- ifelse((ResNet18Prob) < 0.5, "Gibbons", "Noise")#
  # Predict using ResNet50  ResNet50Pred <- predict(modelResNet50Gibbon, test_dl)  ResNet50Prob <- torch_sigmoid(ResNet50Pred)  ResNet50Prob <- as_array(torch_tensor(ResNet50Prob, device = 'cpu'))  ResNet50Class <- ifelse((ResNet50Prob) < 0.5, "Gibbons", "Noise")#
  # Predict using ResNet152  ResNet152Pred <- predict(modelResNet152Gibbon, test_dl)  ResNet152Prob <- torch_sigmoid(ResNet152Pred)  ResNet152Prob <- as_array(torch_tensor(ResNet152Prob, device = 'cpu'))  ResNet152Class <- ifelse((ResNet152Prob) < 0.5, "Gibbons", "Noise")#
  # Add the results to output tables  outputTableResNet18 <- rbind(outputTableResNet18, data.frame(Label = Folder, Probability = ResNet18Prob, PredictedClass = ResNet18Class, ActualClass = Folder))  outputTableResNet50 <- rbind(outputTableResNet50, data.frame(Label = Folder, Probability = ResNet50Prob, PredictedClass = ResNet50Class, ActualClass = Folder))  outputTableResNet152 <- rbind(outputTableResNet152, data.frame(Label = Folder, Probability = ResNet152Prob, PredictedClass = ResNet152Class, ActualClass = Folder))#
  # Save false positives to train next iteration  # file.copy(Folder[which(ResNet18Prob > 0.9)],  #           to = paste('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/data/Temp/Images/Images/', TempShort[which(ResNet18Prob > 0.9)], sep = ''))#
  # Save the output tables as CSV files  write.csv(outputTableResNet18, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet18.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet50, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet50.csv", sep = '_'), row.names = FALSE)  write.csv(outputTableResNet152, paste(output.data.path, trainingfolder, n.epoch, "output_ResNet152.csv", sep = '_'), row.names = FALSE)#
  # Initialize data frames  CombinedTempRow <- data.frame()  TransferLearningCNNDF <- data.frame()#
  # Threshold values to consider  thresholds <- seq(0.1,1,0.1)#
  for (threshold in thresholds) {    # ResNet18    ResNet18PredictedClass <- ifelse((outputTableResNet18$Probability) < threshold, "Gibbons", "Noise")#
    ResNet18Perf <- caret::confusionMatrix(      as.factor(ResNet18PredictedClass),      as.factor(outputTableResNet18$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet18 <- cbind.data.frame(      t(ResNet18Perf[5:7]),      ResNet18.loss,      trainingfolder,      n.epoch,      'ResNet18'    )#
    colnames(TempRowResNet18) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet18$Threshold <- as.character(threshold)#
    # ResNet50    ResNet50PredictedClass <- ifelse((outputTableResNet50$Probability) < threshold, "Gibbons", "Noise")#
    ResNet50Perf <- caret::confusionMatrix(      as.factor(ResNet50PredictedClass),      as.factor(outputTableResNet50$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet50 <- cbind.data.frame(      t(ResNet50Perf[5:7]),      ResNet50.loss,      trainingfolder,      n.epoch,      'ResNet50'    )#
    colnames(TempRowResNet50) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet50$Threshold <- as.character(threshold)#
    # ResNet152    ResNet152PredictedClass <- ifelse((outputTableResNet152$Probability) < threshold, "Gibbons", "Noise")#
    ResNet152Perf <- caret::confusionMatrix(      as.factor(ResNet152PredictedClass),      as.factor(outputTableResNet152$ActualClass),      mode = 'everything'    )$byClass#
    TempRowResNet152 <- cbind.data.frame(      t(ResNet152Perf[5:7]),      ResNet152.loss,      trainingfolder,      n.epoch,      'ResNet152'    )#
    colnames(TempRowResNet152) <- c(      "Precision",      "Recall",      "F1",      "Validation loss",      "Training Data",      "N epochs",      "CNN Architecture"    )#
    TempRowResNet152$Threshold <- as.character(threshold)#
    CombinedTempRowThreshold <- rbind.data.frame(TempRowResNet18, TempRowResNet50, TempRowResNet152)    CombinedTempRowThreshold$Threshold <- as.character(threshold)#
    # Append to the overall result data frame    CombinedTempRow <- rbind.data.frame(CombinedTempRow, CombinedTempRowThreshold)  }#
  # Append to the main data frame  TransferLearningCNNDF <- rbind.data.frame(TransferLearningCNNDF, CombinedTempRow)  TransferLearningCNNDF$Frozen <- unfreeze.param  # Write the result to a CSV file  filename <- paste(output.data.path,'performance_tables/', trainingfolder, '_', n.epoch, '_', '_TransferLearningCNNDFResNet.csv', sep = '')  write.csv(TransferLearningCNNDF, filename, row.names = FALSE)#
  rm(modelResNet18Gibbon)  rm(modelResNet50Gibbon)  rm(modelResNet152Gibbon)}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)TrainedModelInput <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/_output_unfrozen_TRUE_imagesmalaysia_'TrainedModels <- list.files(TrainedModelInput,           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files(TrainedModelInput,                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 1:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)TrainedModelInput <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/_output_unfrozen_TRUE_imagesmalaysia_'TrainedModels <- list.files(TrainedModelInput,           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files(TrainedModelInput,                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/Gibbons'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 1:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}#performance_scores <- read.csv('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/performance_scores_dffrozen.csv')performance_scores <- read.csv('/Users/denaclink/Desktop/RStudioProjects/Multi-species-detector/performance_scores_dfunfrozen_randomsamples.csv')unique_training_data <- unique(performance_scores$TrainingData)performance_scores <- subset(performance_scores, Threshold == .5   )best_f1_results <- data.frame()# Loop through each 'TrainingData' type and find the row with the maximum F1 scorefor (td in unique_training_data) {  subset_data <- subset(performance_scores, TrainingData == td   )  max_f1_row <- subset_data[which.
max(subset_data$F1), ]  best_f1_results <- rbind.data.frame(best_f1_results, max_f1_row)}# Print the best F1 scores for each 'TrainingData'print(best_f1_results)unique_training_data <- unique(performance_scores$TrainingData)unique_model_type <- unique(performance_scores$ModelType)best_f1_results_comb <- data.frame()# Loop through each 'TrainingData' type and find the row with the maximum F1 scorefor (a in 1:length(unique_model_type)) {  TempDF <-  subset(performance_scores, ModelType == unique_model_type[a])#
  for (b in 1:length(unique_training_data)) {    subset_data <- subset(TempDF, TrainingData == unique_training_data[b])    max_f1_row <- subset_data[which.max(subset_data$F1), ]    best_f1_results_comb <- rbind.data.frame(best_f1_results_comb, max_f1_row)  }#
}# Print the best F1 scores for each 'TrainingData'print(best_f1_results_comb)ggpubr::ggboxplot(data=performance_scores,color='ModelType',y='F1',x='Threshold',facet.by = 'TrainingData')#
performance_scores_sub <- subset(performance_scores, NEpochs=='20')performance_scores_sub$Recall <- round(as.numeric(performance_scores_sub$Recall),1)ggpubr::ggline(data=performance_scores_sub,color='ModelType',y='F1',x='Threshold',facet.by = 'TrainingData')#ggpubr::ggline(data=performance_scores_sub,color='ModelType',y='Precision',x='Recall',facet.by = 'TrainingData')
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)TrainedModelInput <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/_output_unfrozen_TRUE_imagesmalaysia_'TrainedModels <- list.files(TrainedModelInput,           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files(TrainedModelInput,                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/Gibbons'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 10:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)TrainedModelInput <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/_output_unfrozen_TRUE_imagesmalaysia_'TrainedModels <- list.files(TrainedModelInput,           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files(TrainedModelInput,                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/Gibbons'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in 1:length(TrainedModels)){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- 1-as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Load required librarieslibrary(luz)                # Luz library for deep learning with PyTorchlibrary(torch)              # PyTorch librarylibrary(torchvision)        # PyTorch library for computer vision taskslibrary(torchdatasets)      # PyTorch library for handling datasetslibrary(stringr)            # String manipulation librarylibrary(tuneR)              # Library for audio analysislibrary(seewave)            # Library for sound analysisset.seed(13)TrainedModelInput <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/_output_unfrozen_TRUE_imagesmalaysia_'TrainedModels <- list.files(TrainedModelInput,           pattern = '.pt',full.names = T)TrainedModelsShort <- list.files(TrainedModelInput,                                pattern = '.pt',full.names = F)#
all.data.input <- '/Volumes/DJC 1TB/VocalIndividualityClips/RandomSelectionImages/Gibbons'image.file.paths <- list.files(all.data.input,recursive = T,full.names = T)image.file.paths.short <- list.files(all.data.input,recursive = T,full.names = F)#
for(a in rev(1:length(TrainedModels))){  # Initialize vectors to store results  performance_scores <- data.frame()#
 modelTempModelGibbons <-  luz_load(TrainedModels[a]) TempModelShort <- TrainedModelsShort[a]   TrainingData <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,2] NEpochs  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,3] ModelType  <- str_split_fixed(TempModelShort, pattern = '_', n=4)[,4] ModelType  <- str_split_fixed(ModelType, pattern = '.pt', n=2)[,1]#
   for(m in 1:length(image.file.paths) ){   unlink('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons', recursive = TRUE)#
   dir.create('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons')#
  file.copy(image.file.paths[m],           to= paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/Gibbons/',                     image.file.paths.short[m],sep=''))#
 ActualLabels <- 'Gibbons'#
 if(str_detect(ModelType,pattern='ResNet') ==F){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_resize(size = c(224, 224)) %>%       torchvision::transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)),     target_transform = function(x) as.double(x) - 1   )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   # Sample data (you should replace this with your actual data)   # Assuming you have a vector of true labels (actual values) and a vector of predicted probabilities   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
     PerformanceTemp$TrainingData <- TrainingData     PerformanceTemp$NEpochs <- NEpochs     PerformanceTemp$ModelType <- ModelType     PerformanceTemp$FilePath <- image.file.paths.short[m]     # Calculate F1 score     performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 if(str_detect(ModelType,pattern='ResNet') ==T){#
   test_ds <- image_folder_dataset(     file.path('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/Temp/Images/'),     transform = . %>%       torchvision::transform_to_tensor() %>%       torchvision::transform_color_jitter() %>%       transform_resize(256) %>%       transform_center_crop(224) %>%       transform_normalize(mean = c(0.485, 0.456, 0.406), std = c(0.229, 0.224, 0.225)), target_transform = function(x) as.double(x) - 1 )#
   # Predict the test files   # Variable indicating the number of files   nfiles <- test_ds$.length()#
   # Load the test images   test_dl <- dataloader(test_ds, batch_size = nfiles)#
   # Predict using TempModel   TempModelPred <- predict(modelTempModelGibbons, test_dl)   TempModelProb <- torch_sigmoid(TempModelPred)   TempModelProb <- as_array(torch_tensor(TempModelProb, device = 'cpu'))#
   true_labels <- ActualLabels   predicted_probs <- TempModelProb   PerformanceTemp <- cbind.data.frame(true_labels,predicted_probs)#
   PerformanceTemp$TrainingData <- TrainingData   PerformanceTemp$NEpochs <- NEpochs   PerformanceTemp$ModelType <- ModelType   PerformanceTemp$FilePath <- image.file.paths.short[m]   # Calculate F1 score   performance_scores <- rbind.data.frame( performance_scores,PerformanceTemp )   }#
 print(PerformanceTemp)  TempName <-  paste('/Users/denaclink/Desktop/RStudioProjects/torch-for-R-PAM-data/data/playbackperformance_single/', PerformanceTemp$ModelType, '_', PerformanceTemp$NEpochs, '_', TrainingData,'_', 'performance_scores_dfunfrozen_playbacks_randomsamples.csv',sep='') write.csv(performance_scores,TempName, row.names = F) } gc()}
# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)# Utils -------------------------------------------------------------------# plotting callbackplot_callback <- luz_callback(  name = "plot",  on_fit_begin = function() {    latent_dim <- ctx$model$latent_dim    self$noise <- torch_randn(1, latent_dim, device = self$ctx$accelerator$device)  },  on_epoch_end = function() {    img <- ctx$model$G(self$noise)    img <- img$cpu()    img <- (img[1,1,,,newaxis] + 1)/2    img <- torch_stack(list(img, img, img), dim = 3)[..,1]    img <- as.raster(as_array(img))    plot(img)  })# Datasets and loaders ----------------------------------------------------input.data.path <- 'data/imagesmalaysia'# Data loaders setuptransform_func <- . %>%  torchvision::transform_to_tensor()train_ds <- image_folder_dataset(file.path(input.data.path, 'test'), transform = transform_func)test_ds <- image_folder_dataset(file.path(input.data.p
ath, 'test'), transform = transform_func)train_dl <- dataloader(train_ds, batch_size = 4, shuffle = FALSE, drop_last = TRUE)test_dl <- dataloader(test_ds, batch_size = 4, shuffle = FALSE, drop_last = TRUE)# Define the network ------------------------------------------------------init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}generator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 512, kernel_size = 4,                          stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(512),      nn_relu(),      nn_conv_transpose2d(512, 256, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_co
nv_transpose2d(256, 128, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4,                          stride = 2, padding = 3, bias = FALSE),      nn_tanh()    )    self$main$apply(init_weights) # custom weight initialization  },  forward = function(input) {    input <- input$view(c(input$shape, 1, 1))    self$main(input)  })discriminator <- nn_module(  "discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 16, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(16, 32, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(32),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(32, 64, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(64),      nn_leaky_relu(0.2, inplace = TR
UE),      nn_conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$main$apply(init_weights) # custom weight initialization    self$linear <- nn_linear(128, 1)  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    x[,1]  })dcgan <- torch::nn_module(  initialize = function(latent_dim = 100, channels = 3) {    self$latent_dim <- latent_dim    self$channels <- channels    self$G <- generator(latent_dim = latent_dim, out_channels = channels)    self$D <- discriminator(in_channels = 3)    self$bce <- torch::nn_bce_with_logits_loss()  },  set_optimizers = function(lr = 2*1e-4, betas = c(0.5, 0.999)) {    list(      discriminator = optim_adam(self$D$parameters, lr = lr, betas = betas),      generator = optim_adam(self$G$parameters, lr = lr, betas = betas)    )  },  loss = function(input, ...) {    # generate a fake image    batch_size <- inpu
t$shape[1]    device <- input$device    noise <- torch_randn(batch_size, self$latent_dim, device = device)    fake <- self$G(noise)    # create response vectors    y_real <- torch_ones(batch_size, device = device)    y_fake <- torch_zeros(batch_size, device = device)    # return different loss depending on the optimizer    if (ctx$opt_name == "discriminator")      self$bce(self$D(input), y_real) + self$bce(self$D(fake$detach()), y_fake)    else if (ctx$opt_name == "generator")      self$bce(self$D(fake), y_real)  })res <- dcgan %>%  setup() %>%  set_hparams(latent_dim = 100, channels = 3) %>%  fit(train_dl, epochs = 10, valid_data = test_dl, callbacks = list(plot_callback()))# Generate picture -------------------------------------------------noise <- torch_randn(1, 100, device = res$model$G$parameters[[1]]$device)img <- res$model$G(noise)img <- img$cpu()img <- (img[1,1,,,newaxis] + 1)/2img <- torch_stack(list(img, img, img), dim = 3)[..,1]img <- as.raster(as_array(img))plot(im
g)# Serialization ----------------------------------------------------#luz_save(res, "mnist-dcgan.pt")
# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)# Utils -------------------------------------------------------------------# plotting callbackplot_callback <- luz_callback(  name = "plot",  on_fit_begin = function() {    latent_dim <- ctx$model$latent_dim    self$noise <- torch_randn(1, latent_dim, device = self$ctx$accelerator$device)  },  on_epoch_end = function() {    img <- ctx$model$G(self$noise)    img <- img$cpu()    img <- (img[1,1,,,newaxis] + 1)/2    img <- torch_stack(list(img, img, img), dim = 3)[..,1]    img <- as.raster(as_array(img))    plot(img)  })# Datasets and loaders ----------------------------------------------------input.data.path <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysia'# Data loaders setuptransform_func <- . %>%  torchvision::transform_to_tensor()train_ds <- image_folder_dataset(file.path(input.data.path, 'test'), transform = transform_func)te
st_ds <- image_folder_dataset(file.path(input.data.path, 'test'), transform = transform_func)train_dl <- dataloader(train_ds, batch_size = 4, shuffle = FALSE, drop_last = TRUE)test_dl <- dataloader(test_ds, batch_size = 4, shuffle = FALSE, drop_last = TRUE)# Define the network ------------------------------------------------------init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}generator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 512, kernel_size = 4,                          stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(512),      nn_relu(),      nn_conv_transpose2d(512, 256, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),
nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4,                          stride = 2, padding = 3, bias = FALSE),      nn_tanh()    )    self$main$apply(init_weights) # custom weight initialization  },  forward = function(input) {    input <- input$view(c(input$shape, 1, 1))    self$main(input)  })discriminator <- nn_module(  "discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 16, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(16, 32, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(32),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(32, 64, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_bat
ch_norm2d(64),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$main$apply(init_weights) # custom weight initialization    self$linear <- nn_linear(128, 1)  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    x[,1]  })dcgan <- torch::nn_module(  initialize = function(latent_dim = 100, channels = 3) {    self$latent_dim <- latent_dim    self$channels <- channels    self$G <- generator(latent_dim = latent_dim, out_channels = channels)    self$D <- discriminator(in_channels = 3)    self$bce <- torch::nn_bce_with_logits_loss()  },  set_optimizers = function(lr = 2*1e-4, betas = c(0.5, 0.999)) {    list(      discriminator = optim_adam(self$D$parameters, lr = lr, betas = betas),      generator = optim_adam(self$G$parameters, lr = lr, betas = betas)    )  },  loss = function(input, ...)
{    # generate a fake image    batch_size <- input$shape[1]    device <- input$device    noise <- torch_randn(batch_size, self$latent_dim, device = device)    fake <- self$G(noise)    # create response vectors    y_real <- torch_ones(batch_size, device = device)    y_fake <- torch_zeros(batch_size, device = device)    # return different loss depending on the optimizer    if (ctx$opt_name == "discriminator")      self$bce(self$D(input), y_real) + self$bce(self$D(fake$detach()), y_fake)    else if (ctx$opt_name == "generator")      self$bce(self$D(fake), y_real)  })res <- dcgan %>%  setup() %>%  set_hparams(latent_dim = 100, channels = 3) %>%  fit(train_dl, epochs = 10, valid_data = test_dl, callbacks = list(plot_callback()))# Generate picture -------------------------------------------------noise <- torch_randn(1, 100, device = res$model$G$parameters[[1]]$device)img <- res$model$G(noise)img <- img$cpu()img <- (img[1,1,,,newaxis] + 1)/2img <- torch_stack(list(img, img, img), di
m = 3)[..,1]img <- as.raster(as_array(img))plot(img)# Serialization ----------------------------------------------------#luz_save(res, "mnist-dcgan.pt")
# Packages ----------------------------------------------------------------library(luz)library(torch)library(torchvision)# Utils -------------------------------------------------------------------# plotting callbackplot_callback <- luz_callback(  name = "plot",  on_fit_begin = function() {    latent_dim <- ctx$model$latent_dim    self$noise <- torch_randn(1, latent_dim, device = self$ctx$accelerator$device)  },  on_epoch_end = function() {    img <- ctx$model$G(self$noise)    img <- img$cpu()    img <- (img[1,1,,,newaxis] + 1)/2    img <- torch_stack(list(img, img, img), dim = 3)[..,1]    img <- as.raster(as_array(img))    plot(img)  })# Datasets and loaders ----------------------------------------------------input.data.path <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysia/test/'train_ds <- image_folder_dataset(  file.path(input.data.path ),  transform = . %>%    torchvision::transform_to_tensor() %>%    transform_resize(28)  )train_dl <- dataloa
der(train_ds, batch_size = 16, shuffle = FALSE, drop_last = TRUE)test_dl <- dataloader(train_ds, batch_size = 16, shuffle = FALSE, drop_last = TRUE)train_ds[1]# Define the network ------------------------------------------------------init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}generator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 256, kernel_size = 4,                          stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size
= 4,                          stride = 2, padding = 3, bias = FALSE),      nn_tanh()    )#
    self$main$apply(init_weights) # custom weight initialization  },  forward = function(input) {    input <- input$view(c(input$shape, 1, 1))    self$main(input)  })discriminator <- nn_module(  "discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 16, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(16, 32, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(32),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(32, 64, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(64),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$main$apply(init_weights) # custom weight initialization    self$linear <- nn_linear(128, 1)  },  forward = function(input) {    x <- self$main(input)    x
<- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    x[,1]  })dcgan <- torch::nn_module(  initialize = function(latent_dim = 100, channels = 3) {    self$latent_dim <- latent_dim    self$channels <- channels    self$G <- generator(latent_dim = latent_dim, out_channels = channels)    self$D <- discriminator(in_channels = 3)    self$bce <- torch::nn_bce_with_logits_loss()  },  set_optimizers = function(lr = 2*1e-4, betas = c(0.5, 0.999)) {    list(      discriminator = optim_adam(self$D$parameters, lr = lr, betas = betas),      generator = optim_adam(self$G$parameters, lr = lr, betas = betas)    )  },  loss = function(input, ...) {    # generate a fake image    batch_size <- input$shape[1]    device <- input$device    noise <- torch_randn(batch_size, self$latent_dim, device = device)    fake <- self$G(noise)    # create response vectors    y_real <- torch_ones(batch_size, device = device)    y_fake <- torch_zeros(batch_size, device = device)    # return different los
s depending on the optimizer    if (ctx$opt_name == "discriminator")      self$bce(self$D(input), y_real) + self$bce(self$D(fake$detach()), y_fake)    else if (ctx$opt_name == "generator")      self$bce(self$D(fake), y_real)  })res <- dcgan %>%  setup() %>%  set_hparams(latent_dim = 100, channels = 3) %>%  fit(train_dl, epochs = 10, valid_data = test_dl, callbacks = list(plot_callback()))# Generate picture -------------------------------------------------noise <- torch_randn(1, 100, device = res$model$G$parameters[[1]]$device)img <- res$model$G(noise)img <- img$cpu()img <- (img[1,1,,,newaxis] + 1)/2img <- torch_stack(list(img, img, img), dim = 3)[..,1]img <- as.raster(as_array(img))plot(img)# Serialization ----------------------------------------------------#luz_save(res, "mnist-dcgan.pt")
# Packages ----------------------------------------------------------------library(torch)library(torchvision)par(mfrow = c(4,4), mar = rep(0.2, 4))# Utils -------------------------------------------------------------------# plots an image generated given the# intermediate stateplot_gen <- function(noise) {  img <- G(noise)  # Generate the image using the generator  img <- img$cpu()  # Move the image tensor to the CPU#
  # Normalize the image to be in the range [0, 1]  img <- (img + 1) / 2#
  # Drop the batch dimension, keep the tensor as 3D [C, H, W]  img <- img[1,,,]#
  # Convert the tensor to an array and permute the dimensions  img <- as_array(img)  img <- aperm(img, c(2, 3, 1))  # Reorder dimensions to [H, W, C]#
  # Convert to raster and plot  img <- as.raster(img)  plot(img)}#
# Datasets and loaders ----------------------------------------------------train_transforms <- function(img) {  img %>%    transform_to_tensor() %>%    transform_resize(c(28, 28)) }#
# Labeled data from BLED detectorinput.data <-  "data/imagesmalaysia/"# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data, "Misassigned"),  transform = train_transforms)#
train_ds$.length()#
class_names <- train_ds$classeslength(class_names)batch_size <- 12train_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = FALSE)train_dl$.length()#
b <- train_dl$.iter()$.next()classes <- b[[2]]dim(b[[1]]) # 12  1 28 28#
# Define the network ------------------------------------------------------# For the generatorgenerator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 1024, kernel_size = 4, stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(1024),      nn_relu(),      nn_conv_transpose2d(1024, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_relu(),      nn_conv_transpose2d(512, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_tanh()    )  },  forward = function(input) {    self$main(input)  })# For the discriminatordiscriminator <- nn_module(
"discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(512, 1024, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$linear <- nn_linear(1024, 1)    self$sigmoid <- nn_sigmoid()  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    self$sigmoid(x)  })#
device <- torch_device(ifelse(cuda_is_available(),  "cuda", "cpu"))G <- generator(latent_dim = 100, out_channels = 3)D <- discriminator(in_channels = 3)init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}G[[1]]$apply(init_weights)D[[1]]$apply(init_weights)G$to(device = device)D$to(device = device)G_optimizer <- optim_adam(G$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))D_optimizer <- optim_adam(D$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))fixed_noise <- torch_randn(3, 100, 1, 1, device = device)dim(fixed_noise)# Training loop -----------------------------------------------------------loss <- nn_bce_loss()img_list <- list()img_num <- 0for (epoch in 1:50000) {#
  pb <- progress::progress_bar$new(    total = length(train_dl),    format = "[:bar] :eta Loss D: :lossd Loss G: :lossg"  )  lossg <- c()  lossd <- c()#
  coro::loop(for (b in train_dl) { tryCatch({ #
    y_real <- torch_ones(12, device = device)    y_fake <- torch_zeros(12, device = device)#
    noise <- torch_randn(12, 100, 1, 1, device = device)    fake <- G(noise)#
    img <- b[[1]]$to(device = device)#
    # train the discriminator ---    D_loss <- loss(D(img), y_real) + loss(D(fake$detach()), y_fake)#
    D_optimizer$zero_grad()    D_loss$backward()    D_optimizer$step()#
    # train the generator ---#
    G_loss <- loss(D(fake), y_real)#
    G_optimizer$zero_grad()    G_loss$backward()    G_optimizer$step()#
    lossd <- c(lossd, D_loss$item())    lossg <- c(lossg, G_loss$item())    pb$tick(tokens = list(lossd = mean(lossd), lossg = mean(lossg)))  }, error = function(e) {    cat("ERROR :", conditionMessage(e), "\n")  })  })#
  with_no_grad({    img_num <- img_num + 1    plot_gen(fixed_noise)    generated <- G(fixed_noise)    grid <- vision_make_grid(generated)    img_list[[img_num]] <- as_array(grid$to(device = "cpu"))  })#
  cat(sprintf("Epoch %d - Loss D: %3f Loss G: %3f\n", epoch, mean(lossd), mean(lossg)))}index <- seq(1, length(img_list), length.out = 16)images <- img_list[index]#
rasterize <- function(x) {  as.raster(x[1, , ])}images %>%  purrr::map(rasterize) %>%  purrr::iwalk(~{plot(.x)})
# Packages ----------------------------------------------------------------library(torch)library(torchvision)par(mfrow = c(4,4), mar = rep(0.2, 4))# Utils -------------------------------------------------------------------# plots an image generated given the# intermediate stateplot_gen <- function(noise) {  img <- G(noise)  # Generate the image using the generator  img <- img$cpu()  # Move the image tensor to the CPU#
  # Normalize the image to be in the range [0, 1]  img <- (img + 1) / 2#
  # Drop the batch dimension, keep the tensor as 3D [C, H, W]  img <- img[1,,,]#
  # Convert the tensor to an array and permute the dimensions  img <- as_array(img)  img <- aperm(img, c(2, 3, 1))  # Reorder dimensions to [H, W, C]#
  # Convert to raster and plot  img <- as.raster(img)  plot(img)}#
# Datasets and loaders ----------------------------------------------------train_transforms <- function(img) {  img %>%    transform_to_tensor() %>%    transform_resize(c(28, 28)) }#
# Labeled data from BLED detectorinput.data <-  "/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysia/"# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data, "Misassigned"),  transform = train_transforms)#
train_ds$.length()#
class_names <- train_ds$classeslength(class_names)batch_size <- 12train_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = FALSE)train_dl$.length()#
b <- train_dl$.iter()$.next()classes <- b[[2]]dim(b[[1]]) # 12  1 28 28#
# Define the network ------------------------------------------------------# For the generatorgenerator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 1024, kernel_size = 4, stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(1024),      nn_relu(),      nn_conv_transpose2d(1024, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_relu(),      nn_conv_transpose2d(512, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_tanh()    )  },  forward = function(input) {    self$main(input)  })# For the discriminatordiscriminator <- nn_module(
"discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(512, 1024, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$linear <- nn_linear(1024, 1)    self$sigmoid <- nn_sigmoid()  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    self$sigmoid(x)  })#
device <- torch_device(ifelse(cuda_is_available(),  "cuda", "cpu"))G <- generator(latent_dim = 100, out_channels = 3)D <- discriminator(in_channels = 3)init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}G[[1]]$apply(init_weights)D[[1]]$apply(init_weights)G$to(device = device)D$to(device = device)G_optimizer <- optim_adam(G$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))D_optimizer <- optim_adam(D$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))fixed_noise <- torch_randn(3, 100, 1, 1, device = device)dim(fixed_noise)# Training loop -----------------------------------------------------------loss <- nn_bce_loss()img_list <- list()img_num <- 0for (epoch in 1:50000) {#
  pb <- progress::progress_bar$new(    total = length(train_dl),    format = "[:bar] :eta Loss D: :lossd Loss G: :lossg"  )  lossg <- c()  lossd <- c()#
  coro::loop(for (b in train_dl) { tryCatch({ #
    y_real <- torch_ones(12, device = device)    y_fake <- torch_zeros(12, device = device)#
    noise <- torch_randn(12, 100, 1, 1, device = device)    fake <- G(noise)#
    img <- b[[1]]$to(device = device)#
    # train the discriminator ---    D_loss <- loss(D(img), y_real) + loss(D(fake$detach()), y_fake)#
    D_optimizer$zero_grad()    D_loss$backward()    D_optimizer$step()#
    # train the generator ---#
    G_loss <- loss(D(fake), y_real)#
    G_optimizer$zero_grad()    G_loss$backward()    G_optimizer$step()#
    lossd <- c(lossd, D_loss$item())    lossg <- c(lossg, G_loss$item())    pb$tick(tokens = list(lossd = mean(lossd), lossg = mean(lossg)))  }, error = function(e) {    cat("ERROR :", conditionMessage(e), "\n")  })  })#
  with_no_grad({    img_num <- img_num + 1    plot_gen(fixed_noise)    generated <- G(fixed_noise)    grid <- vision_make_grid(generated)    img_list[[img_num]] <- as_array(grid$to(device = "cpu"))  })#
  cat(sprintf("Epoch %d - Loss D: %3f Loss G: %3f\n", epoch, mean(lossd), mean(lossg)))}index <- seq(1, length(img_list), length.out = 16)images <- img_list[index]#
rasterize <- function(x) {  as.raster(x[1, , ])}images %>%  purrr::map(rasterize) %>%  purrr::iwalk(~{plot(.x)})
# Packages ----------------------------------------------------------------library(torch)library(torchvision)par(mfrow = c(4,4), mar = rep(0.2, 4))# Utils -------------------------------------------------------------------# plots an image generated given the# intermediate stateplot_gen <- function(noise) {  img <- G(noise)  # Generate the image using the generator  img <- img$cpu()  # Move the image tensor to the CPU#
  # Normalize the image to be in the range [0, 1]  img <- (img + 1) / 2#
  # Drop the batch dimension, keep the tensor as 3D [C, H, W]  img <- img[1,,,]#
  # Convert the tensor to an array and permute the dimensions  img <- as_array(img)  img <- aperm(img, c(2, 3, 1))  # Reorder dimensions to [H, W, C]#
  # Convert to raster and plot  img <- as.raster(img)  plot(img)}#
# Datasets and loaders ----------------------------------------------------train_transforms <- function(img) {  img %>%    transform_to_tensor() %>%    transform_resize(c(28, 28)) }#
# Labeled data from BLED detectorinput.data <-  "/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysia/"# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data, "Misassigned"),  transform = train_transforms)#
train_ds$.length()#
class_names <- train_ds$classeslength(class_names)batch_size <- 12train_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = FALSE,drop_last = TRUE)train_dl$.length()#
b <- train_dl$.iter()$.next()classes <- b[[2]]dim(b[[1]]) # 12  1 28 28#
# Define the network ------------------------------------------------------# For the generatorgenerator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 1024, kernel_size = 4, stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(1024),      nn_relu(),      nn_conv_transpose2d(1024, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_relu(),      nn_conv_transpose2d(512, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_tanh()    )  },  forward = function(input) {    self$main(input)  })# For the discriminatordiscriminator <- nn_module(
"discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(512, 1024, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$linear <- nn_linear(1024, 1)    self$sigmoid <- nn_sigmoid()  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    self$sigmoid(x)  })#
device <- torch_device(ifelse(cuda_is_available(),  "cuda", "cpu"))G <- generator(latent_dim = 100, out_channels = 3)D <- discriminator(in_channels = 3)init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}G[[1]]$apply(init_weights)D[[1]]$apply(init_weights)G$to(device = device)D$to(device = device)G_optimizer <- optim_adam(G$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))D_optimizer <- optim_adam(D$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))fixed_noise <- torch_randn(3, 100, 1, 1, device = device)dim(fixed_noise)# Training loop -----------------------------------------------------------loss <- nn_bce_loss()img_list <- list()img_num <- 0for (epoch in 1:50000) {#
  pb <- progress::progress_bar$new(    total = length(train_dl),    format = "[:bar] :eta Loss D: :lossd Loss G: :lossg"  )  lossg <- c()  lossd <- c()#
  coro::loop(for (b in train_dl) { tryCatch({ #
    y_real <- torch_ones(12, device = device)    y_fake <- torch_zeros(12, device = device)#
    noise <- torch_randn(12, 100, 1, 1, device = device)    fake <- G(noise)#
    img <- b[[1]]$to(device = device)#
    # train the discriminator ---    D_loss <- loss(D(img), y_real) + loss(D(fake$detach()), y_fake)#
    D_optimizer$zero_grad()    D_loss$backward()    D_optimizer$step()#
    # train the generator ---#
    G_loss <- loss(D(fake), y_real)#
    G_optimizer$zero_grad()    G_loss$backward()    G_optimizer$step()#
    lossd <- c(lossd, D_loss$item())    lossg <- c(lossg, G_loss$item())    pb$tick(tokens = list(lossd = mean(lossd), lossg = mean(lossg)))  }, error = function(e) {    cat("ERROR :", conditionMessage(e), "\n")  })  })#
  with_no_grad({    img_num <- img_num + 1    plot_gen(fixed_noise)    generated <- G(fixed_noise)    grid <- vision_make_grid(generated)    img_list[[img_num]] <- as_array(grid$to(device = "cpu"))  })#
  cat(sprintf("Epoch %d - Loss D: %3f Loss G: %3f\n", epoch, mean(lossd), mean(lossg)))}index <- seq(1, length(img_list), length.out = 16)images <- img_list[index]#
rasterize <- function(x) {  as.raster(x[1, , ])}images %>%  purrr::map(rasterize) %>%  purrr::iwalk(~{plot(.x)})
# Packages ----------------------------------------------------------------library(torch)library(torchvision)par(mfrow = c(4,4), mar = rep(0.2, 4))# Utils -------------------------------------------------------------------# plots an image generated given the# intermediate stateplot_gen <- function(noise) {  img <- G(noise)  # Generate the image using the generator  img <- img$cpu()  # Move the image tensor to the CPU#
  # Normalize the image to be in the range [0, 1]  img <- (img + 1) / 2#
  # Drop the batch dimension, keep the tensor as 3D [C, H, W]  img <- img[1,,,]#
  # Convert the tensor to an array and permute the dimensions  img <- as_array(img)  img <- aperm(img, c(2, 3, 1))  # Reorder dimensions to [H, W, C]#
  # Convert to raster and plot  img <- as.raster(img)  plot(img)}#
# Datasets and loaders ----------------------------------------------------train_transforms <- function(img) {  img %>%    transform_to_tensor() %>%    transform_resize(c(28, 28)) }#
# Labeled data from BLED detectorinput.data <-  "/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysiamulti/"# Combined uses bothtrain_ds <- image_folder_dataset(  file.path(input.data, "testnonoise"),  transform = train_transforms)#
train_ds$.length()#
class_names <- train_ds$classeslength(class_names)batch_size <- 10train_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = FALSE,drop_last = TRUE)train_dl$.length()train_dl$batch_size# Define the network ------------------------------------------------------# For the generatorgenerator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 256, kernel_size = 4, stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_tanh()    )  },  forward = function(input) {    self$main(input)  })#
# For the discriminatordiscriminator <- nn_module(  "discriminator",  initialize = function(in_channels) {    self$main <- nn_sequential(      nn_conv2d(in_channels, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(512),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(512, 1024, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$linear <- nn_linear(1024, 1)    self$sigmoid <- nn_sigmoid()  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    self$sigmoid(x)  })#
device <- torch_device(ifelse(cuda_is_available(),  "cuda", "cpu"))G <- generator(latent_dim = 100, out_channels = 3)D <- discriminator(in_channels = 3)init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}G[[1]]$apply(init_weights)D[[1]]$apply(init_weights)G$to(device = device)D$to(device = device)G_optimizer <- optim_adam(G$parameters, lr = (2 * 1e-4) * 0.5 , betas = c(0.5, 0.999))D_optimizer <- optim_adam(D$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))fixed_noise <- torch_randn(3, 100, 1, 1, device = device)dim(fixed_noise)# Training loop -----------------------------------------------------------loss <- nn_bce_loss()img_list <- list()img_num <- 0for (epoch in 1:50000) {#
  pb <- progress::progress_bar$new(    total = length(train_dl),    format = "[:bar] :eta Loss D: :lossd Loss G: :lossg"  )  lossg <- c()  lossd <- c()#
  coro::loop(for (b in train_dl) { tryCatch({ #
    batch_size <- train_dl$batch_size  # Extract the batch size from the real images    y_real <- torch_ones(batch_size, device = device)    y_fake <- torch_zeros(batch_size, device = device)#
    noise <- torch_randn(batch_size, 100, 1, 1, device = device)    fake <- G(noise)#
    img <- b[[1]]$to(device = device)#
    # train the discriminator ---    D_loss <- loss(D(img), y_real) + loss(D(fake$detach()), y_fake)#
    D_optimizer$zero_grad()    D_loss$backward()    D_optimizer$step()#
    # train the generator ---    G_loss <- loss(D(fake), y_real)#
    G_optimizer$zero_grad()    G_loss$backward()    G_optimizer$step()#
    lossd <- c(lossd, D_loss$item())    lossg <- c(lossg, G_loss$item())    pb$tick(tokens = list(lossd = mean(lossd), lossg = mean(lossg)))  }, error = function(e) {    cat("ERROR :", conditionMessage(e), "\n")  })  })#
  with_no_grad({    img_num <- img_num + 1    plot_gen(fixed_noise)    generated <- G(fixed_noise)    grid <- vision_make_grid(generated)    img_list[[img_num]] <- as_array(grid$to(device = "cpu"))  })#
  cat(sprintf("Epoch %d - Loss D: %3f Loss G: %3f\n", epoch, mean(lossd), mean(lossg)))}index <- seq(1, length(img_list), length.out = 16)images <- img_list[index]#
rasterize <- function(x) {  as.raster(x[1, , ])}images %>%  purrr::map(rasterize) %>%  purrr::iwalk(~{plot(.x)})
# Packages ----------------------------------------------------------------library(torch)library(torchvision)par(mfrow = c(4,4), mar = rep(0.2, 4))# Utils -------------------------------------------------------------------#
# Datasets and loaders ----------------------------------------------------# Load dplyr package for data manipulation functionslibrary(dplyr)batch_size <- 20# Location of spectrogram images for traininginput.data.path <-  '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysiamulti/'# Create a dataset of images:# - The images are sourced from the 'train' subdirectory within the specified path `input.data.path`.# - The images undergo several transformations:#     1. They are converted to tensors.#     2. They are resized to 224x224 pixels.#     3. Their pixel values are normalized.train_ds <- image_folder_dataset(  file.path(input.data.path,'train' ),   # Path to the image directory  transform = . %>%    torchvision::transform_to_tensor() %>%    torchvision::transform_resize(size = c(28,28)) %>%    torchvision::transform_normalize(      mean = c(0.485, 0.456, 0.406),      # Mean for normalization      std = c(0.229, 0.224, 0.225)        # Standard deviation for normalizat
ion    ))# Create a dataloader from the dataset:# - This helps in efficiently loading and batching the data.# - The batch size is set to 24, with shuffling enabled and the last incomplete batch is dropped.train_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = TRUE, drop_last = TRUE)# Extract the next batch from the dataloaderbatch <- train_dl$.iter()$.next()# Extract the labels for the batch and determine class namesclasses <- batch[[2]]#class_names <- ifelse(batch$y, 'Noise','Gibbons')# Convert the batch tensor of images to an array and process them:# - The image tensor is permuted to change the dimension order.# - The pixel values of the images are denormalized.images <- as_array(batch[[1]]) %>% aperm(perm = c(1, 3, 4, 2))mean <- c(0.485, 0.456, 0.406)std <- c(0.229, 0.224, 0.225)images <- std * images + meanimages <- images * 255# Clip the pixel values to lie within [0, 255]images[images > 255] <- 255images[images < 0] <- 0# Set the plotting parameters for a 4x6 g
ridpar(mfcol = c(4,6), mar = rep(1, 4))# Visualize the images:# - Use `purrr` functions to handle arrays.# - Set the name of each image based on its class.# - Convert each image to a raster format for plotting.# - Finally, iterate over each image, plotting it and setting its title.images %>%  purrr::array_tree(1) %>%  #purrr::set_names(class_names) %>%  purrr::map(as.raster, max = 255) %>%  purrr::iwalk(~{plot(.x); title(.y)})#
# Utils -------------------------------------------------------------------# plots an image generated given the# intermediate state  plot_gen <- function(noise) {    img <- G(noise)    img <- img$cpu()    img <- img[1,,,]/2 + 0.5    img <- aperm(as.array(img), c(2, 3, 1))    img <- as.raster((img))    plot(img)  }#
# Define the network ------------------------------------------------------generator <- nn_module(  "generator",  initialize = function(latent_dim, out_channels) {    self$main <- nn_sequential(      nn_conv_transpose2d(latent_dim, 512, kernel_size = 4,                          stride = 1, padding = 0, bias = FALSE),      nn_batch_norm2d(512),      nn_relu(),      nn_conv_transpose2d(512, 256, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(256),      nn_relu(),      nn_conv_transpose2d(256, 128, kernel_size = 4,                          stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(128),      nn_relu(),      nn_conv_transpose2d(128, out_channels, kernel_size = 4,                          stride = 2, padding = 3, bias = FALSE),      nn_tanh()    )  },  forward = function(input) {    self$main(input)  })discriminator <- nn_module(  "discriminator",  initialize = function(in_channels) {    self$main <- nn_sequ
ential(      nn_conv2d(in_channels, 16, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(16, 32, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(32),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(32, 64, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_batch_norm2d(64),      nn_leaky_relu(0.2, inplace = TRUE),      nn_conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1, bias = FALSE),      nn_leaky_relu(0.2, inplace = TRUE)    )    self$linear <- nn_linear(128, 1)    self$sigmoid <- nn_sigmoid()  },  forward = function(input) {    x <- self$main(input)    x <- torch_flatten(x, start_dim = 2)    x <- self$linear(x)    self$sigmoid(x)  })device <- torch_device(ifelse(cuda_is_available(),  "cuda", "cpu"))G <- generator(latent_dim = 100, out_channels = 3)D <- discriminator(in_channels = 3)init_weights <- function(m) {  if (grepl("conv", m$.classes[[1]])) {
nn_init_normal_(m$weight$data(), 0.0, 0.02)  } else if (grepl("batch_norm", m$.classes[[1]])) {    nn_init_normal_(m$weight$data(), 1.0, 0.02)    nn_init_constant_(m$bias$data(), 0)  }}G[[1]]$apply(init_weights)D[[1]]$apply(init_weights)G$to(device = device)D$to(device = device)G_optimizer <- optim_adam(G$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))D_optimizer <- optim_adam(D$parameters, lr = 2 * 1e-4, betas = c(0.5, 0.999))# Training loop -----------------------------------------------------------loss <- nn_bce_loss()for (epoch in 1:100) {  pb <- progress::progress_bar$new(    total = length(train_dl),    format = "[:bar] :eta Loss D: :lossd Loss G: :lossg"  )  lossg <- c()  lossd <- c()  coro::loop(for (b in train_dl) {    y_real <- torch_ones(batch_size, device = device)    y_fake <- torch_zeros(batch_size, device = device)    noise <- torch_randn(batch_size, 100, 1, 1, device = device)    fake <- G(noise)    img <- b[[1]]$to(device = device)    # train the di
scriminator ---    D_loss <- loss(D(img), y_real) + loss(D(fake$detach()), y_fake)    D_loss <- torch_tensor(D_loss, device = device, requires_grad = TRUE)    D_optimizer$zero_grad()    D_loss$backward()    D_optimizer$step()    # train the generator ---    G_loss <- loss(D(fake), y_real)    G_loss <- torch_tensor(G_loss, device = device, requires_grad = TRUE)    G_optimizer$zero_grad()    G_loss$backward()    G_optimizer$step()    lossd <- c(lossd, D_loss$item())    lossg <- c(lossg, G_loss$item())    pb$tick(tokens = list(lossd = mean(lossd), lossg = mean(lossg)))  })  with_no_grad({    plot_gen(noise)  })  cat(sprintf("Epoch %d - Loss D: %3f Loss G: %3f\n", epoch, mean(lossd), mean(lossg)))}
library(dplyr)setwd("/Users/denaclink/Desktop/RStudioProjects/gibbonNetR")devtools::document()devtools::load_all("/Users/denaclink/Desktop/RStudioProjects/gibbonNetR")#
# Multi-class models using 'train_CNN_multi' ------------------------------# Location of spectrogram images for traininginput.data.path <-  'data/imagesmalaysiamulti/'# Location of spectrogram images for testingtest.data.path <- '/Users/denaclink/Desktop/RStudioProjects/gibbonNetR/data/imagesmalaysiamaliau copy/test/'# Training data folder shorttrainingfolder.short <- 'imagesmalaysiamulti'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Allow early stopping?early.stop <- 'yes'gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                             architecture ='alexnet',                             learning_rate = 0.001,                             test.data=test.data.path,                             unfreeze = TRUE,                             epoch.iterations=epoch.iterations,                             save.model= TRUE,                             early.stop = "yes",                             output.base.path = "data/test/",                             trainingfolder=trainingfolder.short,                             noise.category = "noise")gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                            architecture ='vgg16',                            learning_rate = 0.001,                            test.data=test.data.path,                            unfreeze = TRUE,                            epoch.iterations=epoch.iterations,                            save.model= TRUE,                            early.stop = "yes",                            output.base.path = "data/test/",                            trainingfolder=trainingfolder.short,                            noise.category = "noise")gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                            architecture ='vgg19',                            learning_rate = 0.001,                            test.data=test.data.path,                            unfreeze = TRUE,                            epoch.iterations=epoch.iterations,                            save.model= TRUE,                            early.stop = "yes",                            output.base.path = "data/test/",                            trainingfolder=trainingfolder.short,                            noise.category = "noise")gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                            architecture ='resnet18',                            learning_rate = 0.001,                            test.data=test.data.path,                            unfreeze = TRUE,                            epoch.iterations=epoch.iterations,                            save.model= TRUE,                            early.stop = "yes",                            output.base.path = "data/test/",                            trainingfolder=trainingfolder.short,                            noise.category = "noise")gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                            architecture ='resnet50',                            learning_rate = 0.001,                            test.data=test.data.path,                            unfreeze = TRUE,                            epoch.iterations=epoch.iterations,                            save.model= TRUE,                            early.stop = "yes",                            output.base.path = "data/test/",                            trainingfolder=trainingfolder.short,                            noise.category = "noise")gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                            architecture ='resnet152',                            learning_rate = 0.001,                            test.data=test.data.path,                            unfreeze = TRUE,                            epoch.iterations=epoch.iterations,                            save.model= TRUE,                            early.stop = "yes",                            output.base.path = "data/test/",                            trainingfolder=trainingfolder.short,                            noise.category = "noise")
gibbonNetR::train_CNN_multi(input.data.path=input.data.path,                            architecture ='resnet152',                            learning_rate = 0.001,                            test.data=test.data.path,                            unfreeze = TRUE,                            epoch.iterations=c(4,5,20),                            save.model= TRUE,                            early.stop = "yes",                            output.base.path = "data/test/",                            trainingfolder=trainingfolder.short,                            noise.category = "noise")
# Location of spectrogram images for traininginput.data.path <-  'data/imagesmalaysia/'# Location of spectrogram images for testingtest.data.path <- 'data/imagesmalaysiamaliau/test/'# Training data folder shorttrainingfolder.short <- 'imagesmalaysia'# Whether to unfreeze the layersunfreeze.param <- TRUE # FALSE means the features are frozen; TRUE unfrozen# Number of epochs to includeepoch.iterations <- c(1,2,3,4,5,20)# Allow early stopping?early.stop <- 'yes'gibbonNetR::train_CNN_binary(input.data.path=input.data.path,                          architecture ='alexnet',                          learning_rate = 0.001,                          save.model= TRUE,                          test.data=test.data.path,                          unfreeze = TRUE,                          epoch.iterations=epoch.iterations,                          early.stop = "yes",                          output.base.path = "data/test/",                          trainingfolder=trainingfolder.short,                          positive.class="Gibbons",                          negative.class="Noise")gibbonNetR::train_CNN_binary(input.data.path=input.data.path,                             architecture ='vgg16',                             learning_rate = 0.001,                             save.model= TRUE,                             test.data=test.data.path,                             unfreeze = TRUE,                             epoch.iterations=epoch.iterations,                             early.stop = "yes",                             output.base.path = "data/test/",                             trainingfolder=trainingfolder.short,                             positive.class="Gibbons",                             negative.class="Noise")gibbonNetR::train_CNN_binary(input.data.path=input.data.path,                             architecture ='vgg19',                             learning_rate = 0.001,                             test.data=test.data.path,                             save.model= TRUE,                             unfreeze = TRUE,                             epoch.iterations=epoch.iterations,                             early.stop = "yes",                             output.base.path = "data/test/",                             trainingfolder=trainingfolder.short,                             positive.class="Gibbons",                             negative.class="Noise")gibbonNetR::train_CNN_binary(input.data.path=input.data.path,                             architecture ='resnet18',                             learning_rate = 0.001,                             test.data=test.data.path,                             save.model= TRUE,                             unfreeze = TRUE,                             epoch.iterations=epoch.iterations,                             early.stop = "yes",                             output.base.path = "data/test/",                             trainingfolder=trainingfolder.short,                             positive.class="Gibbons",                             negative.class="Noise")gibbonNetR::train_CNN_binary(input.data.path=input.data.path,                             architecture ='resnet50',                             learning_rate = 0.001,                             test.data=test.data.path,                             save.model= TRUE,                             unfreeze = TRUE,                             epoch.iterations=epoch.iterations,                             early.stop = "yes",                             output.base.path = "data/test/",                             trainingfolder=trainingfolder.short,                             positive.class="Gibbons",                             negative.class="Noise")gibbonNetR::train_CNN_binary(input.data.path=input.data.path,                             architecture ='resnet152',                             save.model= TRUE,                             learning_rate = 0.001,                             test.data=test.data.path,                             unfreeze = TRUE,                             epoch.iterations=epoch.iterations,                             early.stop = "yes",                             output.base.path = "data/test/",                             trainingfolder=trainingfolder.short,                             positive.class="Gibbons",                             negative.class="Noise")
